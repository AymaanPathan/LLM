{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98030af-fcd1-4d63-a36e-38ba053498fa",
   "metadata": {},
   "source": [
    "# A full business solution\n",
    "\n",
    "## Now we will take our project from Day 1 to the next level\n",
    "\n",
    "### BUSINESS CHALLENGE:\n",
    "\n",
    "Create a product that builds a Brochure for a company to be used for prospective clients, investors and potential recruits.\n",
    "\n",
    "We will be provided a company name and their primary website.\n",
    "\n",
    "See the end of this notebook for examples of real-world business applications.\n",
    "\n",
    "And remember: I'm always available if you have problems or ideas! Please do reach out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5b08506-dc8b-4443-9201-5f1848161363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# If these fail, please check you're running from an 'activated' environment with (llms) in the command prompt\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc5d8880-f2ee-4c06-af16-ecbc0262af61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Hello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with whatever you need. How can I assist you today? üòä\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "message = [\n",
    "    {\"role\":\"user\" , \"content\":\"how are you\"}\n",
    "]\n",
    "response = chat(model=\"deepseek-r1:1.5b\",messages=message)\n",
    "print(response['message'][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "106dd65e-90af-4ca8-86b6-23a41840645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "\n",
    "# Some websites need you to use proper headers when fetching them:\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "    \"\"\"\n",
    "    A utility class to represent a Website that we have scraped, now with links\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url):  \n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        self.body = response.content\n",
    "        soup = BeautifulSoup(self.body, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        if soup.body:\n",
    "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevant.decompose()\n",
    "            self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            self.text = \"\"\n",
    "        links = [link.get('href') for link in soup.find_all('a')]\n",
    "        self.links = [link for link in links if link]\n",
    "\n",
    "    def get_contents(self):\n",
    "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30d8128-933b-44cc-81c8-ab4c9d86589a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage Title:\n",
      "Home - Edward Donner\n",
      "Webpage Contents:\n",
      "Home\n",
      "Connect Four\n",
      "Outsmart\n",
      "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
      "About\n",
      "Posts\n",
      "Well, hi there.\n",
      "I‚Äôm Ed. I like writing code and experimenting with LLMs, and hopefully you‚Äôre here because you do too. I also enjoy DJing (but I‚Äôm badly out of practice), amateur electronic music production (\n",
      "very\n",
      "amateur) and losing myself in\n",
      "Hacker News\n",
      ", nodding my head sagely to things I only half understand.\n",
      "I‚Äôm the co-founder and CTO of\n",
      "Nebula.io\n",
      ". We‚Äôre applying AI to a field where it can make a massive, positive impact: helping people discover their potential and pursue their reason for being. Recruiters use our product today to source, understand, engage and manage talent. I‚Äôm previously the founder and CEO of AI startup untapt,\n",
      "acquired in 2021\n",
      ".\n",
      "We work with groundbreaking, proprietary LLMs verticalized for talent, we‚Äôve\n",
      "patented\n",
      "our matching model, and our award-winning platform has happy customers and tons of press coverage.\n",
      "Connect\n",
      "with me for more!\n",
      "April 21, 2025\n",
      "The Complete Agentic AI Engineering Course\n",
      "January 23, 2025\n",
      "LLM Workshop ‚Äì Hands-on with Agents ‚Äì resources\n",
      "December 21, 2024\n",
      "Welcome, SuperDataScientists!\n",
      "November 13, 2024\n",
      "Mastering AI and LLM Engineering ‚Äì Resources\n",
      "Navigation\n",
      "Home\n",
      "Connect Four\n",
      "Outsmart\n",
      "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
      "About\n",
      "Posts\n",
      "Get in touch\n",
      "ed [at] edwarddonner [dot] com\n",
      "www.edwarddonner.com\n",
      "Follow me\n",
      "LinkedIn\n",
      "Twitter\n",
      "Facebook\n",
      "Subscribe to newsletter\n",
      "Type your email‚Ä¶\n",
      "Subscribe\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ed = Website(\"https://edwarddonner.com\")\n",
    "data =ed.get_contents()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1771af9c-717a-4fca-bbbe-8a95893312c3",
   "metadata": {},
   "source": [
    "## First step: Have GPT-4o-mini figure out which links are relevant\n",
    "\n",
    "### Use a call to gpt-4o-mini to read the links on a webpage, and respond in structured JSON.  \n",
    "It should decide which links are relevant, and replace relative links such as \"/about\" with \"https://company.com/about\".  \n",
    "We will use \"one shot prompting\" in which we provide an example of how it should respond in the prompt.\n",
    "\n",
    "This is an excellent use case for an LLM, because it requires nuanced understanding. Imagine trying to code this without LLMs by parsing and analyzing the webpage - it would be very hard!\n",
    "\n",
    "Sidenote: there is a more advanced technique called \"Structured Outputs\" in which we require the model to respond according to a spec. We cover this technique in Week 8 during our autonomous Agentic AI project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6957b079-0d96-45f7-a26a-3487510e9b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_system_prompt = (\n",
    "    \"You are provided with a list of links found on a webpage. \"\n",
    "    \"You must decide which links would be most relevant to include in a brochure about the company, \"\n",
    "    \"such as links to About, Company, Careers, or Jobs pages.\\n\\n\"\n",
    "    \"You MUST respond with ONLY a valid JSON object and nothing else ‚Äî no explanations, no thoughts, no text, no markdown formatting (no triple backticks).\\n\\n\"\n",
    "    \"The JSON should look like this:\\n\\n\"\n",
    "    \"{\\n\"\n",
    "    \"    \\\"links\\\": [\\n\"\n",
    "    \"        {\\\"type\\\": \\\"about page\\\", \\\"url\\\": \\\"https://full.url/goes/here/about\\\"},\\n\"\n",
    "    \"        {\\\"type\\\": \\\"careers page\\\", \\\"url\\\": \\\"https://another.full.url/careers\\\"}\\n\"\n",
    "    \"    ]\\n\"\n",
    "    \"}\\n\\n\"\n",
    "    \"‚ö†Ô∏è IMPORTANT: Only output the JSON object. Do NOT include any commentary, explanation, thinking, or markdown code blocks.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b97e4068-97ed-4120-beae-c42105e4d59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are provided with a list of links found on a webpage. You must decide which links would be most relevant to include in a brochure about the company, such as links to About, Company, Careers, or Jobs pages.\n",
      "\n",
      "You MUST respond with ONLY a valid JSON object and nothing else ‚Äî no explanations, no thoughts, no text, no markdown formatting (no triple backticks).\n",
      "\n",
      "The JSON should look like this:\n",
      "\n",
      "{\n",
      "    \"links\": [\n",
      "        {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
      "        {\"type\": \"careers page\", \"url\": \"https://another.full.url/careers\"}\n",
      "    ]\n",
      "}\n",
      "\n",
      "‚ö†Ô∏è IMPORTANT: Only output the JSON object. Do NOT include any commentary, explanation, thinking, or markdown code blocks.\n"
     ]
    }
   ],
   "source": [
    "print(link_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e1f601b-2eaf-499d-b6b8-c99050c9d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_user_prompt(website):\n",
    "    user_prompt = f\"Here is the list of links on the website of {website.url} - \"\n",
    "    user_prompt += \"please decide which of these are relevant web links for a brochure about the company, respond with the full https URL in JSON format. \\\n",
    "Do not include Terms of Service, Privacy, email links.\\n\"\n",
    "    user_prompt += \"Links (some might be relative links):\\n\"\n",
    "    user_prompt += \"\\n\".join(website.links)\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bcbfa78-6395-4685-b92c-22d592050fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the list of links on the website of https://edwarddonner.com - please decide which of these are relevant web links for a brochure about the company, respond with the full https URL in JSON format. Do not include Terms of Service, Privacy, email links.\n",
      "Links (some might be relative links):\n",
      "https://edwarddonner.com/\n",
      "https://edwarddonner.com/connect-four/\n",
      "https://edwarddonner.com/outsmart/\n",
      "https://edwarddonner.com/about-me-and-about-nebula/\n",
      "https://edwarddonner.com/posts/\n",
      "https://edwarddonner.com/\n",
      "https://news.ycombinator.com\n",
      "https://nebula.io/?utm_source=ed&utm_medium=referral\n",
      "https://www.prnewswire.com/news-releases/wynden-stark-group-acquires-nyc-venture-backed-tech-startup-untapt-301269512.html\n",
      "https://patents.google.com/patent/US20210049536A1/\n",
      "https://www.linkedin.com/in/eddonner/\n",
      "https://edwarddonner.com/2025/04/21/the-complete-agentic-ai-engineering-course/\n",
      "https://edwarddonner.com/2025/04/21/the-complete-agentic-ai-engineering-course/\n",
      "https://edwarddonner.com/2025/01/23/llm-workshop-hands-on-with-agents-resources/\n",
      "https://edwarddonner.com/2025/01/23/llm-workshop-hands-on-with-agents-resources/\n",
      "https://edwarddonner.com/2024/12/21/llm-resources-superdatascience/\n",
      "https://edwarddonner.com/2024/12/21/llm-resources-superdatascience/\n",
      "https://edwarddonner.com/2024/11/13/llm-engineering-resources/\n",
      "https://edwarddonner.com/2024/11/13/llm-engineering-resources/\n",
      "https://edwarddonner.com/\n",
      "https://edwarddonner.com/connect-four/\n",
      "https://edwarddonner.com/outsmart/\n",
      "https://edwarddonner.com/about-me-and-about-nebula/\n",
      "https://edwarddonner.com/posts/\n",
      "mailto:hello@mygroovydomain.com\n",
      "https://www.linkedin.com/in/eddonner/\n",
      "https://twitter.com/edwarddonner\n",
      "https://www.facebook.com/edward.donner.52\n"
     ]
    }
   ],
   "source": [
    "print(get_links_user_prompt(ed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a29aca19-ca13-471c-a4b4-5abbfa813f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "import json\n",
    "import re\n",
    "\n",
    "def get_links(url):\n",
    "    website = Website(url)\n",
    "    response = chat(\n",
    "        model=\"deepseek-r1:1.5b\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": link_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_links_user_prompt(website)}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    # Extract the content\n",
    "    result = response.get(\"message\", {}).get(\"content\", \"\")\n",
    "    \n",
    "    if result:\n",
    "        # Use regular expression to find the JSON part in the response\n",
    "        match = re.search(r'\\{.*\\}', result, re.DOTALL)  # This regex matches the JSON part\n",
    "        \n",
    "        if match:\n",
    "            json_content = match.group(0)  # Get the matched JSON block\n",
    "            try:\n",
    "                # Now try parsing the JSON content\n",
    "                return json.loads(json_content)\n",
    "            except json.JSONDecodeError:\n",
    "                # If parsing fails, provide more information\n",
    "                raise ValueError(f\"Failed to parse content as JSON. The matched content is: {json_content}\")\n",
    "        else:\n",
    "            raise ValueError(\"No valid JSON found in the response.\")\n",
    "    else:\n",
    "        raise ValueError(\"The response content is empty or invalid.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74a827a0-2782-4ae5-b210-4a242a8b4cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/',\n",
       " '/models',\n",
       " '/datasets',\n",
       " '/spaces',\n",
       " '/posts',\n",
       " '/docs',\n",
       " '/enterprise',\n",
       " '/pricing',\n",
       " '/login',\n",
       " '/join',\n",
       " '/spaces',\n",
       " '/models',\n",
       " '/nari-labs/Dia-1.6B',\n",
       " '/sand-ai/MAGI-1',\n",
       " '/microsoft/bitnet-b1.58-2B-4T',\n",
       " '/ostris/Flex.2-preview',\n",
       " '/HiDream-ai/HiDream-I1-Full',\n",
       " '/models',\n",
       " '/spaces/nari-labs/Dia-1.6B',\n",
       " '/spaces/enzostvs/deepsite',\n",
       " '/spaces/InstantX/InstantCharacter',\n",
       " '/spaces/bytedance-research/UNO-FLUX',\n",
       " '/spaces/Kwai-Kolors/Kolors-Virtual-Try-On',\n",
       " '/spaces',\n",
       " '/datasets/nvidia/OpenMathReasoning',\n",
       " '/datasets/Anthropic/values-in-the-wild',\n",
       " '/datasets/zwhe99/DeepMath-103K',\n",
       " '/datasets/OpenGVLab/InternVL-Data',\n",
       " '/datasets/nvidia/OpenCodeReasoning',\n",
       " '/datasets',\n",
       " '/join',\n",
       " '/pricing#endpoints',\n",
       " '/pricing#spaces',\n",
       " '/pricing',\n",
       " '/enterprise',\n",
       " '/enterprise',\n",
       " '/enterprise',\n",
       " '/enterprise',\n",
       " '/enterprise',\n",
       " '/enterprise',\n",
       " '/enterprise',\n",
       " '/allenai',\n",
       " '/facebook',\n",
       " '/amazon',\n",
       " '/google',\n",
       " '/Intel',\n",
       " '/microsoft',\n",
       " '/grammarly',\n",
       " '/Writer',\n",
       " '/docs/transformers',\n",
       " '/docs/diffusers',\n",
       " '/docs/safetensors',\n",
       " '/docs/huggingface_hub',\n",
       " '/docs/tokenizers',\n",
       " '/docs/trl',\n",
       " '/docs/transformers.js',\n",
       " '/docs/smolagents',\n",
       " '/docs/peft',\n",
       " '/docs/datasets',\n",
       " '/docs/text-generation-inference',\n",
       " '/docs/accelerate',\n",
       " '/models',\n",
       " '/datasets',\n",
       " '/spaces',\n",
       " '/tasks',\n",
       " 'https://ui.endpoints.huggingface.co',\n",
       " '/chat',\n",
       " '/huggingface',\n",
       " '/brand',\n",
       " '/terms-of-service',\n",
       " '/privacy',\n",
       " 'https://apply.workable.com/huggingface/',\n",
       " 'mailto:press@huggingface.co',\n",
       " '/learn',\n",
       " '/docs',\n",
       " '/blog',\n",
       " 'https://discuss.huggingface.co',\n",
       " 'https://status.huggingface.co/',\n",
       " 'https://github.com/huggingface',\n",
       " 'https://twitter.com/huggingface',\n",
       " 'https://www.linkedin.com/company/huggingface/',\n",
       " '/join/discord']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anthropic has made their site harder to scrape, so I'm using HuggingFace..\n",
    "\n",
    "huggingface = Website(\"https://huggingface.co\")\n",
    "huggingface.links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d3d583e2-dcc4-40cc-9b28-1e8dbf402924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'links': [{'type': 'models', 'url': '//models'},\n",
       "  {'type': 'spaces', 'url': '/spaces'},\n",
       "  {'type': 'datasets', 'url': '/datasets'},\n",
       "  {'type': 'posts', 'url': '/posts'},\n",
       "  {'type': 'docs', 'url': '/docs'},\n",
       "  {'type': 'enterprise', 'url': '/enterprise'},\n",
       "  {'type': 'pricing#endpoints', 'url': '/pricing#endpoints'},\n",
       "  {'type': 'Terms-of-Service', 'url': '/terms-of-service'},\n",
       "  {'type': 'Privacy', 'url': '/privacy'},\n",
       "  {'type': 'mail', 'url': 'mailto:press@huggingface.co'}]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_links(\"https://huggingface.co\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74128e-dfb6-47ec-9549-288b621c838c",
   "metadata": {},
   "source": [
    "## Second step: make the brochure!\n",
    "\n",
    "Assemble all the details into another prompt to GPT4-o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "85a5b6e2-e7ef-44a9-bc7f-59ede71037b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found links: {'links': [{'type': 'about page', 'url': '/models'}, {'type': 'about page', 'url': '/datasets'}, {'type': 'about page', 'url': '/spaces'}, {'type': 'about page', 'url': '/nari-labs/Dia-1.6B'}, {'type': 'about page', 'url': '/sand-ai/MAGI-1'}, {'type': 'about page', 'url': '/Microsoft/bitnet-b1.58-2T'}, {'type': 'about page', 'url': '/ostris/Flex.2-preview'}, {'type': 'about page', 'url': '/HiDream-ai/HiDream-I1-Full'}, {'type': 'about page', 'url': '/spaces/nari-labs/Dia-1.6B'}, {'type': 'about page', 'url': '/spaces/enzostvs/deepsite'}, {'type': 'about page', 'url': '/spaces/Kwai-Kolors/Kolors-Virtual-Try-On'}, {'type': 'about page', 'url': '/spaces'}, {'type': 'about page', 'url': '/spaces/datasets/nvidia/OpenMathReasoning'}, {'type': 'about page', 'url': '/spaces/Anthropic/values-in-the-wild'}, {'type': 'about page', 'url': '/spaces/zwhe99/DeepMath-103K'}, {'type': 'about page', 'url': '/spaces/OpenGVLab/InternVL-Data'}, {'type': 'about page', 'url': '/spaces/nvidia/OpenCodeReasoning'}, {'type': 'about page', 'url': '/spaces/join/pricing#endpoints'}, {'type': 'about page', 'url': '/pricing#spaces'}, {'type': 'about page', 'url': '/pricing'}, {'type': 'about page', 'url': '/enterprise/allenai/facebook/amazon/google/Intel/microsoft/grammarly/Writer/docs(transformers/docs/diffusers/docs/safetensors/docs/huggingface_hub/docs/tokenizers/docs/trl/docs/transformers.js/docs/smol agents/docs/peft/docs/datasets/docs/text-generation-inference/docs/accelerate/models/datasets/spaces/tasks'}, {'type': 'about page', 'url': '/alliance/join/discord'}]}\n",
      "Landing page:\n",
      "Webpage Title:\n",
      "Hugging Face ‚Äì The AI community building the future.\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "The AI community building the future.\n",
      "The platform where the machine learning community collaborates on models, datasets, and applications.\n",
      "Explore AI Apps\n",
      "or\n",
      "Browse 1M+ models\n",
      "Trending on\n",
      "this week\n",
      "Models\n",
      "nari-labs/Dia-1.6B\n",
      "Updated\n",
      "1 day ago\n",
      "‚Ä¢\n",
      "63.3k\n",
      "‚Ä¢\n",
      "1.17k\n",
      "sand-ai/MAGI-1\n",
      "Updated\n",
      "1 day ago\n",
      "‚Ä¢\n",
      "394\n",
      "microsoft/bitnet-b1.58-2B-4T\n",
      "Updated\n",
      "3 days ago\n",
      "‚Ä¢\n",
      "32.1k\n",
      "‚Ä¢\n",
      "798\n",
      "ostris/Flex.2-preview\n",
      "Updated\n",
      "about 21 hours ago\n",
      "‚Ä¢\n",
      "3.85k\n",
      "‚Ä¢\n",
      "203\n",
      "HiDream-ai/HiDream-I1-Full\n",
      "Updated\n",
      "5 days ago\n",
      "‚Ä¢\n",
      "31.2k\n",
      "‚Ä¢\n",
      "747\n",
      "Browse 1M+ models\n",
      "Spaces\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "700\n",
      "700\n",
      "Dia 1.6B\n",
      "üëØ\n",
      "Generate realistic dialogue from a script, using Dia!\n",
      "Running\n",
      "5.32k\n",
      "5.32k\n",
      "DeepSite\n",
      "üê≥\n",
      "Generate any application with DeepSeek\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "255\n",
      "255\n",
      "InstantCharacter\n",
      "üê¢\n",
      "Customize characters with prompts and styles\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "586\n",
      "586\n",
      "UNO FLUX\n",
      "‚ö°\n",
      "Generate customized images using text and multiple images\n",
      "Running\n",
      "on\n",
      "CPU Upgrade\n",
      "8.53k\n",
      "8.53k\n",
      "Kolors Virtual Try-On\n",
      "üëï\n",
      "Try on clothes on images of people\n",
      "Browse 400k+ applications\n",
      "Datasets\n",
      "nvidia/OpenMathReasoning\n",
      "Updated\n",
      "2 days ago\n",
      "‚Ä¢\n",
      "7.29k\n",
      "‚Ä¢\n",
      "102\n",
      "Anthropic/values-in-the-wild\n",
      "Updated\n",
      "6 days ago\n",
      "‚Ä¢\n",
      "285\n",
      "‚Ä¢\n",
      "102\n",
      "zwhe99/DeepMath-103K\n",
      "Updated\n",
      "8 days ago\n",
      "‚Ä¢\n",
      "14.9k\n",
      "‚Ä¢\n",
      "152\n",
      "OpenGVLab/InternVL-Data\n",
      "Updated\n",
      "8 minutes ago\n",
      "‚Ä¢\n",
      "1.95k\n",
      "‚Ä¢\n",
      "70\n",
      "nvidia/OpenCodeReasoning\n",
      "Updated\n",
      "11 days ago\n",
      "‚Ä¢\n",
      "13.1k\n",
      "‚Ä¢\n",
      "300\n",
      "Browse 250k+ datasets\n",
      "The Home of Machine Learning\n",
      "Create, discover and collaborate on ML better.\n",
      "The collaboration platform\n",
      "Host and collaborate on unlimited public models, datasets and applications.\n",
      "Move faster\n",
      "With the HF Open source stack.\n",
      "Explore all modalities\n",
      "Text, image, video, audio or even 3D.\n",
      "Build your portfolio\n",
      "Share your work with the world and build your ML profile.\n",
      "Sign Up\n",
      "Accelerate your ML\n",
      "We provide paid Compute and Enterprise solutions.\n",
      "Compute\n",
      "Deploy on optimized\n",
      "Inference Endpoints\n",
      "or update your\n",
      "Spaces applications\n",
      "to a GPU in a few clicks.\n",
      "View pricing\n",
      "Starting at $0.60/hour for GPU\n",
      "Enterprise\n",
      "Give your team the most advanced platform to build AI with enterprise-grade security, access controls and\n",
      "\t\t\tdedicated support.\n",
      "Getting started\n",
      "Starting at $20/user/month\n",
      "Single Sign-On\n",
      "Regions\n",
      "Priority Support\n",
      "Audit Logs\n",
      "Resource Groups\n",
      "Private Datasets Viewer\n",
      "More than 50,000 organizations are using Hugging Face\n",
      "Ai2\n",
      "Enterprise\n",
      "non-profit\n",
      "‚Ä¢\n",
      "749 models\n",
      "‚Ä¢\n",
      "3.12k followers\n",
      "AI at Meta\n",
      "Enterprise\n",
      "company\n",
      "‚Ä¢\n",
      "2.12k models\n",
      "‚Ä¢\n",
      "5.7k followers\n",
      "Amazon\n",
      "company\n",
      "‚Ä¢\n",
      "20 models\n",
      "‚Ä¢\n",
      "3.05k followers\n",
      "Google\n",
      "company\n",
      "‚Ä¢\n",
      "990 models\n",
      "‚Ä¢\n",
      "12.3k followers\n",
      "Intel\n",
      "company\n",
      "‚Ä¢\n",
      "219 models\n",
      "‚Ä¢\n",
      "2.47k followers\n",
      "Microsoft\n",
      "company\n",
      "‚Ä¢\n",
      "372 models\n",
      "‚Ä¢\n",
      "11.7k followers\n",
      "Grammarly\n",
      "Enterprise\n",
      "company\n",
      "‚Ä¢\n",
      "10 models\n",
      "‚Ä¢\n",
      "154 followers\n",
      "Writer\n",
      "Enterprise\n",
      "company\n",
      "‚Ä¢\n",
      "21 models\n",
      "‚Ä¢\n",
      "262 followers\n",
      "Our Open Source\n",
      "We are building the foundation of ML tooling with the community.\n",
      "Transformers\n",
      "143,514\n",
      "State-of-the-art ML for PyTorch, TensorFlow, JAX\n",
      "Diffusers\n",
      "28,750\n",
      "State-of-the-art Diffusion models in PyTorch\n",
      "Safetensors\n",
      "3,244\n",
      "Safe way to store/distribute neural network weights\n",
      "Hub Python Library\n",
      "2,542\n",
      "Python client to interact with the Hugging Face Hub\n",
      "Tokenizers\n",
      "9,627\n",
      "Fast tokenizers optimized for research & production\n",
      "TRL\n",
      "13,439\n",
      "Train transformers LMs with reinforcement learning\n",
      "Transformers.js\n",
      "13,500\n",
      "State-of-the-art ML running directly in your browser\n",
      "smolagents\n",
      "17,571\n",
      "Smol library to build great agents in Python\n",
      "PEFT\n",
      "18,215\n",
      "Parameter-efficient finetuning for large language models\n",
      "Datasets\n",
      "20,027\n",
      "Access & share datasets for any ML tasks\n",
      "Text Generation Inference\n",
      "10,054\n",
      "Serve language models with TGI optimized toolkit\n",
      "Accelerate\n",
      "8,658\n",
      "Train PyTorch models with multi-GPU, TPU, mixed precision\n",
      "System theme\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Tasks\n",
      "Inference Endpoints\n",
      "HuggingChat\n",
      "Company\n",
      "About\n",
      "Brand assets\n",
      "Terms of service\n",
      "Privacy\n",
      "Jobs\n",
      "Press\n",
      "Resources\n",
      "Learn\n",
      "Documentation\n",
      "Blog\n",
      "Forum\n",
      "Service Status\n",
      "Social\n",
      "GitHub\n",
      "Twitter\n",
      "LinkedIn\n",
      "Discord\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "Models - Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Edit Models filters\n",
      "Tasks\n",
      "Libraries\n",
      "Datasets\n",
      "Languages\n",
      "Licenses\n",
      "Other\n",
      "Multimodal\n",
      "Audio-Text-to-Text\n",
      "Image-Text-to-Text\n",
      "Visual Question Answering\n",
      "Document Question Answering\n",
      "Video-Text-to-Text\n",
      "Visual Document Retrieval\n",
      "Any-to-Any\n",
      "Computer Vision\n",
      "Depth Estimation\n",
      "Image Classification\n",
      "Object Detection\n",
      "Image Segmentation\n",
      "Text-to-Image\n",
      "Image-to-Text\n",
      "Image-to-Image\n",
      "Image-to-Video\n",
      "Unconditional Image Generation\n",
      "Video Classification\n",
      "Text-to-Video\n",
      "Zero-Shot Image Classification\n",
      "Mask Generation\n",
      "Zero-Shot Object Detection\n",
      "Text-to-3D\n",
      "Image-to-3D\n",
      "Image Feature Extraction\n",
      "Keypoint Detection\n",
      "Natural Language Processing\n",
      "Text Classification\n",
      "Token Classification\n",
      "Table Question Answering\n",
      "Question Answering\n",
      "Zero-Shot Classification\n",
      "Translation\n",
      "Summarization\n",
      "Feature Extraction\n",
      "Text Generation\n",
      "Text2Text Generation\n",
      "Fill-Mask\n",
      "Sentence Similarity\n",
      "Text Ranking\n",
      "Audio\n",
      "Text-to-Speech\n",
      "Text-to-Audio\n",
      "Automatic Speech Recognition\n",
      "Audio-to-Audio\n",
      "Audio Classification\n",
      "Voice Activity Detection\n",
      "Tabular\n",
      "Tabular Classification\n",
      "Tabular Regression\n",
      "Time Series Forecasting\n",
      "Reinforcement Learning\n",
      "Reinforcement Learning\n",
      "Robotics\n",
      "Other\n",
      "Graph Machine Learning\n",
      "Apply filters\n",
      "Models\n",
      "Full-text search\n",
      "Add filters\n",
      "Sort:¬†\n",
      "\t\tTrending\n",
      "nari-labs/Dia-1.6B\n",
      "Text-to-Speech\n",
      "‚Ä¢\n",
      "Updated\n",
      "1 day ago\n",
      "‚Ä¢\n",
      "63.3k\n",
      "‚Ä¢\n",
      "‚Ä¢\n",
      "1.17k\n",
      "sand-ai/MAGI-1\n",
      "Image-to-Video\n",
      "‚Ä¢\n",
      "Updated\n",
      "1 day ago\n",
      "‚Ä¢\n",
      "394\n",
      "microsoft/bitnet-b1.58-2B-4T\n",
      "Text Generation\n",
      "‚Ä¢\n",
      "Updated\n",
      "3 days ago\n",
      "‚Ä¢\n",
      "32.1k\n",
      "‚Ä¢\n",
      "798\n",
      "ostris/Flex.2-preview\n",
      "Text-to-Image\n",
      "‚Ä¢\n",
      "Updated\n",
      "about 21 hours ago\n",
      "‚Ä¢\n",
      "3.85k\n",
      "‚Ä¢\n",
      "203\n",
      "HiDream-ai/HiDream-I1-Full\n",
      "Text-to-Image\n",
      "‚Ä¢\n",
      "Updated\n",
      "5 days ago\n",
      "‚Ä¢\n",
      "31.2k\n",
      "‚Ä¢\n",
      "‚Ä¢\n",
      "747\n",
      "THUDM/GLM-4-32B-0414\n",
      "Text Generation\n",
      "‚Ä¢\n",
      "Updated\n",
      "12 days ago\n",
      "‚Ä¢\n",
      "8.77k\n",
      "‚Ä¢\n",
      "‚Ä¢\n",
      "282\n",
      "microsoft/MAI-DS-R1\n",
      "Text Generation\n",
      "‚Ä¢\n",
      "Updated\n",
      "4 days ago\n",
      "‚Ä¢\n",
      "6.69k\n",
      "‚Ä¢\n",
      "242\n",
      "ByteDance-Seed/UI-TARS-1.5-7B\n",
      "Image-Text-to-Text\n",
      "‚Ä¢\n",
      "Updated\n",
      "9 days ago\n",
      "‚Ä¢\n",
      "9.75k\n",
      "‚Ä¢\n",
      "165\n",
      "deepseek-ai/DeepSeek-V3-0324\n",
      "Text Generation\n",
      "‚Ä¢\n",
      "Updated\n",
      "about 1 month ago\n",
      "‚Ä¢\n",
      "274k\n",
      "‚Ä¢\n",
      "‚Ä¢\n",
      "2.77k\n",
      "naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B\n",
      "Updated\n",
      "1 day ago\n",
      "‚Ä¢\n",
      "3.93k\n",
      "‚Ä¢\n",
      "105\n",
      "Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0\n",
      "Text-to-Image\n",
      "‚Ä¢\n",
      "Updated\n",
      "2 days ago\n",
      "‚Ä¢\n",
      "18.2k\n",
      "‚Ä¢\n",
      "217\n",
      "Wan-AI/Wan2.1-FLF2V-14B-720P\n",
      "Updated\n",
      "9 days ago\n",
      "‚Ä¢\n",
      "14.5k\n",
      "‚Ä¢\n",
      "176\n",
      "Skywork/Skywork-R1V2-38B\n",
      "Image-Text-to-Text\n",
      "‚Ä¢\n",
      "Updated\n",
      "3 days ago\n",
      "‚Ä¢\n",
      "793\n",
      "‚Ä¢\n",
      "86\n",
      "black-forest-labs/FLUX.1-dev\n",
      "Text-to-Image\n",
      "‚Ä¢\n",
      "Updated\n",
      "Aug 16, 2024\n",
      "‚Ä¢\n",
      "2.22M\n",
      "‚Ä¢\n",
      "‚Ä¢\n",
      "9.95k\n",
      "hexgrad/Kokoro-82M\n",
      "Text-to-Speech\n",
      "‚Ä¢\n",
      "Updated\n",
      "16 days ago\n",
      "‚Ä¢\n",
      "1.99M\n",
      "‚Ä¢\n",
      "‚Ä¢\n",
      "4.13k\n",
      "Qwen/Qwen2.5-Omni-7B\n",
      "Any-to-Any\n",
      "‚Ä¢\n",
      "Updated\n",
      "11 days ago\n",
      "‚Ä¢\n",
      "225k\n",
      "‚Ä¢\n",
      "1.51k\n",
      "bytedance-research/ChatTS-14B\n",
      "Text Generation\n",
      "‚Ä¢\n",
      "Updated\n",
      "3 days ago\n",
      "‚Ä¢\n",
      "986\n",
      "‚Ä¢\n",
      "105\n",
      "OnomaAIResearch/Illustrious-XL-v2.0\n",
      "Updated\n",
      "7 days ago\n",
      "‚Ä¢\n",
      "75\n",
      "bytedance-research/Phantom\n",
      "Updated\n",
      "5 days ago\n",
      "‚Ä¢\n",
      "68\n",
      "deepseek-ai/DeepSeek-R1\n",
      "Text Generation\n",
      "‚Ä¢\n",
      "Updated\n",
      "about 1 month ago\n",
      "‚Ä¢\n",
      "1.75M\n",
      "‚Ä¢\n",
      "‚Ä¢\n",
      "12k\n",
      "nvidia/DAM-3B\n",
      "Image-Text-to-Text\n",
      "‚Ä¢\n",
      "Updated\n",
      "2 days ago\n",
      "‚Ä¢\n",
      "925\n",
      "‚Ä¢\n",
      "66\n",
      "google/gemma-3-27b-it\n",
      "Image-Text-to-Text\n",
      "‚Ä¢\n",
      "Updated\n",
      "Mar 21\n",
      "‚Ä¢\n",
      "395k\n",
      "‚Ä¢\n",
      "‚Ä¢\n",
      "1.26k\n",
      "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B\n",
      "Updated\n",
      "2 days ago\n",
      "‚Ä¢\n",
      "3.07k\n",
      "‚Ä¢\n",
      "62\n",
      "moonshotai/Kimi-Audio-7B-Instruct\n",
      "Text-to-Speech\n",
      "‚Ä¢\n",
      "Updated\n",
      "about 24 hours ago\n",
      "‚Ä¢\n",
      "67\n",
      "‚Ä¢\n",
      "62\n",
      "NousResearch/Minos-v1\n",
      "Text Classification\n",
      "‚Ä¢\n",
      "Updated\n",
      "1 day ago\n",
      "‚Ä¢\n",
      "452\n",
      "‚Ä¢\n",
      "61\n",
      "Skywork/SkyReels-V2-I2V-14B-540P\n",
      "Image-to-Video\n",
      "‚Ä¢\n",
      "Updated\n",
      "1 day ago\n",
      "‚Ä¢\n",
      "413\n",
      "‚Ä¢\n",
      "59\n",
      "microsoft/bitnet-b1.58-2B-4T-gguf\n",
      "Text Generation\n",
      "‚Ä¢\n",
      "Updated\n",
      "4 days ago\n",
      "‚Ä¢\n",
      "23.1k\n",
      "‚Ä¢\n",
      "131\n",
      "stepfun-ai/Step1X-Edit\n",
      "Image-to-Image\n",
      "‚Ä¢\n",
      "Updated\n",
      "about 12 hours ago\n",
      "‚Ä¢\n",
      "57\n",
      "google/gemma-3-27b-it-qat-q4_0-gguf\n",
      "Image-Text-to-Text\n",
      "‚Ä¢\n",
      "Updated\n",
      "16 days ago\n",
      "‚Ä¢\n",
      "68.5k\n",
      "‚Ä¢\n",
      "244\n",
      "lllyasviel/FramePackI2V_HY\n",
      "Updated\n",
      "13 days ago\n",
      "‚Ä¢\n",
      "238k\n",
      "‚Ä¢\n",
      "90\n",
      "System theme\n",
      "Company\n",
      "TOS\n",
      "Privacy\n",
      "About\n",
      "Jobs\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Pricing\n",
      "Docs\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "Hugging Face ‚Äì The AI community building the future.\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Edit Datasets filters\n",
      "Main\n",
      "Tasks\n",
      "Libraries\n",
      "Languages\n",
      "Licenses\n",
      "Other\n",
      "Modalities\n",
      "3D\n",
      "Audio\n",
      "Geospatial\n",
      "Image\n",
      "Tabular\n",
      "Text\n",
      "Time-series\n",
      "Video\n",
      "Size\n",
      "\t\t\t(rows)\n",
      "Reset Size\n",
      "< 1K\n",
      "> 1T\n",
      "Format\n",
      "json\n",
      "csv\n",
      "parquet\n",
      "imagefolder\n",
      "soundfolder\n",
      "webdataset\n",
      "text\n",
      "arrow\n",
      "Apply filters\n",
      "Datasets\n",
      "368,774\n",
      "Full-text search\n",
      "Add filters\n",
      "Sort:¬†\n",
      "\t\tTrending\n",
      "Anthropic/values-in-the-wild\n",
      "Preview\n",
      "‚Ä¢\n",
      "Updated\n",
      "6 days ago\n",
      "‚Ä¢\n",
      "285\n",
      "‚Ä¢\n",
      "102\n",
      "nvidia/OpenMathReasoning\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "2 days ago\n",
      "‚Ä¢\n",
      "5.47M\n",
      "‚Ä¢\n",
      "7.29k\n",
      "‚Ä¢\n",
      "102\n",
      "zwhe99/DeepMath-103K\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "8 days ago\n",
      "‚Ä¢\n",
      "103k\n",
      "‚Ä¢\n",
      "14.9k\n",
      "‚Ä¢\n",
      "152\n",
      "OpenGVLab/InternVL-Data\n",
      "Preview\n",
      "‚Ä¢\n",
      "Updated\n",
      "8 minutes ago\n",
      "‚Ä¢\n",
      "1.95k\n",
      "‚Ä¢\n",
      "70\n",
      "nvidia/OpenCodeReasoning\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "11 days ago\n",
      "‚Ä¢\n",
      "753k\n",
      "‚Ä¢\n",
      "13.1k\n",
      "‚Ä¢\n",
      "300\n",
      "fka/awesome-chatgpt-prompts\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "Jan 6\n",
      "‚Ä¢\n",
      "203\n",
      "‚Ä¢\n",
      "12.3k\n",
      "‚Ä¢\n",
      "7.73k\n",
      "future-technologies/Universal-Transformers-Dataset\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "3 minutes ago\n",
      "‚Ä¢\n",
      "70.1M\n",
      "‚Ä¢\n",
      "5.1k\n",
      "‚Ä¢\n",
      "85\n",
      "openai/mrcr\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "12 days ago\n",
      "‚Ä¢\n",
      "2.4k\n",
      "‚Ä¢\n",
      "3.1k\n",
      "‚Ä¢\n",
      "130\n",
      "JoeYing/ReTool-SFT\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "10 days ago\n",
      "‚Ä¢\n",
      "2k\n",
      "‚Ä¢\n",
      "627\n",
      "‚Ä¢\n",
      "22\n",
      "nvidia/ClimbLab\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "5 days ago\n",
      "‚Ä¢\n",
      "1.1B\n",
      "‚Ä¢\n",
      "10.8k\n",
      "‚Ä¢\n",
      "30\n",
      "marcodsn/academic-chains\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "about 20 hours ago\n",
      "‚Ä¢\n",
      "2.02k\n",
      "‚Ä¢\n",
      "1.19k\n",
      "‚Ä¢\n",
      "20\n",
      "Eureka-Lab/PHYBench\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "about 2 hours ago\n",
      "‚Ä¢\n",
      "500\n",
      "‚Ä¢\n",
      "96\n",
      "‚Ä¢\n",
      "20\n",
      "nvidia/describe-anything-dataset\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "2 days ago\n",
      "‚Ä¢\n",
      "916k\n",
      "‚Ä¢\n",
      "2.73k\n",
      "‚Ä¢\n",
      "18\n",
      "nvidia/Llama-Nemotron-Post-Training-Dataset\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "10 days ago\n",
      "‚Ä¢\n",
      "3.91M\n",
      "‚Ä¢\n",
      "7.94k\n",
      "‚Ä¢\n",
      "427\n",
      "nvidia/ClimbMix\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "4 days ago\n",
      "‚Ä¢\n",
      "355M\n",
      "‚Ä¢\n",
      "1.84k\n",
      "‚Ä¢\n",
      "23\n",
      "openai/graphwalks\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "12 days ago\n",
      "‚Ä¢\n",
      "1.15k\n",
      "‚Ä¢\n",
      "1.25k\n",
      "‚Ä¢\n",
      "65\n",
      "facebook/PE-Video\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "8 days ago\n",
      "‚Ä¢\n",
      "118k\n",
      "‚Ä¢\n",
      "6.06k\n",
      "‚Ä¢\n",
      "20\n",
      "bh2821/LightNovel5000\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "10 days ago\n",
      "‚Ä¢\n",
      "6.12M\n",
      "‚Ä¢\n",
      "742\n",
      "‚Ä¢\n",
      "27\n",
      "newsletter/HiDream-I1-Artists\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "5 days ago\n",
      "‚Ä¢\n",
      "3.89k\n",
      "‚Ä¢\n",
      "3.87k\n",
      "‚Ä¢\n",
      "14\n",
      "allenai/tulu-3-sft-mixture\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "Dec 2, 2024\n",
      "‚Ä¢\n",
      "939k\n",
      "‚Ä¢\n",
      "5.1k\n",
      "‚Ä¢\n",
      "140\n",
      "FreedomIntelligence/medical-o1-reasoning-SFT\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "4 days ago\n",
      "‚Ä¢\n",
      "90.1k\n",
      "‚Ä¢\n",
      "13.7k\n",
      "‚Ä¢\n",
      "661\n",
      "UCSC-VLAA/MedReason\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "about 13 hours ago\n",
      "‚Ä¢\n",
      "32.7k\n",
      "‚Ä¢\n",
      "1.63k\n",
      "‚Ä¢\n",
      "40\n",
      "Skywork/Skywork-OR1-RL-Data\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "11 days ago\n",
      "‚Ä¢\n",
      "119k\n",
      "‚Ä¢\n",
      "1.86k\n",
      "‚Ä¢\n",
      "28\n",
      "a-m-team/AM-DeepSeek-Distilled-40M\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "1 day ago\n",
      "‚Ä¢\n",
      "11.5M\n",
      "‚Ä¢\n",
      "2.03k\n",
      "‚Ä¢\n",
      "18\n",
      "HuggingFaceFW/fineweb\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "Jan 31\n",
      "‚Ä¢\n",
      "25B\n",
      "‚Ä¢\n",
      "836k\n",
      "‚Ä¢\n",
      "2.12k\n",
      "cais/hle\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "23 days ago\n",
      "‚Ä¢\n",
      "2.5k\n",
      "‚Ä¢\n",
      "9.1k\n",
      "‚Ä¢\n",
      "318\n",
      "Congliu/Chinese-DeepSeek-R1-Distill-data-110k\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "Feb 21\n",
      "‚Ä¢\n",
      "110k\n",
      "‚Ä¢\n",
      "2.57k\n",
      "‚Ä¢\n",
      "643\n",
      "facebook/PLM-Video-Human\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "8 days ago\n",
      "‚Ä¢\n",
      "2.8M\n",
      "‚Ä¢\n",
      "1.57k\n",
      "‚Ä¢\n",
      "18\n",
      "qiaojin/PubMedQA\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "Mar 6, 2024\n",
      "‚Ä¢\n",
      "274k\n",
      "‚Ä¢\n",
      "13.5k\n",
      "‚Ä¢\n",
      "216\n",
      "Anthropic/hh-rlhf\n",
      "Viewer\n",
      "‚Ä¢\n",
      "Updated\n",
      "May 26, 2023\n",
      "‚Ä¢\n",
      "169k\n",
      "‚Ä¢\n",
      "15.9k\n",
      "‚Ä¢\n",
      "1.33k\n",
      "Previous\n",
      "1\n",
      "2\n",
      "3\n",
      "...\n",
      "100\n",
      "Next\n",
      "System theme\n",
      "Company\n",
      "TOS\n",
      "Privacy\n",
      "About\n",
      "Jobs\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Pricing\n",
      "Docs\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "Spaces - Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Spaces\n",
      "¬∑\n",
      "The AI App Directory\n",
      "New Space\n",
      "What is Spaces?\n",
      "Image Generation\n",
      "Video Generation\n",
      "Text Generation\n",
      "Language Translation\n",
      "Speech Synthesis\n",
      "3D Modeling\n",
      "Object Detection\n",
      "Text Analysis\n",
      "Image Editing\n",
      "Code Generation\n",
      "Question Answering\n",
      "Data Visualization\n",
      "Voice Cloning\n",
      "Background Removal\n",
      "Image Upscaling\n",
      "OCR\n",
      "Document Analysis\n",
      "Visual QA\n",
      "Image Captioning\n",
      "Chatbots\n",
      "Sentiment Analysis\n",
      "Text Summarization\n",
      "Music Generation\n",
      "Medical Imaging\n",
      "Financial Analysis\n",
      "Game AI\n",
      "Model Benchmarking\n",
      "Fine Tuning Tools\n",
      "Dataset Creation\n",
      "Pose Estimation\n",
      "Face Recognition\n",
      "Anomaly Detection\n",
      "Recommendation Systems\n",
      "Character Animation\n",
      "Style Transfer\n",
      "Image\n",
      "Spaces of the week\n",
      "21 Apr 2025\n",
      "Sort:¬†\n",
      "\t\tRelevance\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "700\n",
      "Dia 1.6B\n",
      "üëØ\n",
      "Generate realistic dialogue from a script, using Dia!\n",
      "nari-labs\n",
      "about 9 hours ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "48\n",
      "Chat with Bitnet-b1.58-2B-4T\n",
      "üëæ\n",
      "Chat with Microsoft's 1.58bit Bitnet model!\n",
      "suayptalha\n",
      "6 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "42\n",
      "Vevo for Zero-shot VC, TTS, and More\n",
      "üê†\n",
      "Controllable Zero-Shot Voice Imitation\n",
      "amphion\n",
      "3 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "20\n",
      "MESA\n",
      "üèî\n",
      "Text-to-terrain model (reflectance and elevation)\n",
      "mikonvergence\n",
      "10 days ago\n",
      "Running\n",
      "66\n",
      "MotionShop2\n",
      "üèÉ\n",
      "Replace characters in a video with characters in photos\n",
      "3DAIGC\n",
      "11 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "49\n",
      "DetailGen3D\n",
      "üëÅ\n",
      "Generate detailed 3D models from reference images and coarse models\n",
      "VAST-AI\n",
      "8 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "54\n",
      "FLUX.1 Dev ControlNet Union Pro 2.0\n",
      "üî•\n",
      "Create images using prompts and control images\n",
      "Shakker-Labs\n",
      "9 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "48\n",
      "AudioX\n",
      "üëÄ\n",
      "Generate audio and video from text prompts\n",
      "Zeyue7\n",
      "10 days ago\n",
      "All running apps, trending first\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "700\n",
      "Dia 1.6B\n",
      "üëØ\n",
      "Generate realistic dialogue from a script, using Dia!\n",
      "nari-labs\n",
      "about 9 hours ago\n",
      "Running\n",
      "5.32k\n",
      "DeepSite\n",
      "üê≥\n",
      "Generate any application with DeepSeek\n",
      "enzostvs\n",
      "9 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "255\n",
      "InstantCharacter\n",
      "üê¢\n",
      "Customize characters with prompts and styles\n",
      "InstantX\n",
      "6 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "586\n",
      "UNO FLUX\n",
      "‚ö°\n",
      "Generate customized images using text and multiple images\n",
      "bytedance-research\n",
      "14 days ago\n",
      "Running\n",
      "on\n",
      "CPU Upgrade\n",
      "8.53k\n",
      "Kolors Virtual Try-On\n",
      "üëï\n",
      "Try on clothes on images of people\n",
      "Kwai-Kolors\n",
      "Sep 18, 2024\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "1.34k\n",
      "EasyControl Ghibli\n",
      "ü¶Ä\n",
      "New Ghibli EasyControl model is now released!!\n",
      "jamesliu1217\n",
      "15 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "95\n",
      "Describe Anything\n",
      "‚ö°\n",
      "Describe parts of images using text prompts\n",
      "nvidia\n",
      "2 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "95\n",
      "FramePackÂõæÂÉèÂà∞ËßÜÈ¢ëÁîüÊàê(5ÁßíÈôêÂà∂Áâà)\n",
      "üé¨\n",
      "Generate video from an image\n",
      "lisonallen\n",
      "7 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "1.53k\n",
      "FLUX LoRa the Explorer\n",
      "üèÜ\n",
      "Generate images based on prompts and LoRA models\n",
      "multimodalart\n",
      "Feb 17\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "300\n",
      "OminiControl Art\n",
      "üé®\n",
      "Transform images into artistic styles like Studio Ghibli\n",
      "Yuanshi\n",
      "5 days ago\n",
      "Running\n",
      "on\n",
      "CPU Upgrade\n",
      "297\n",
      "HiDream I1 Dev\n",
      "üöÄ\n",
      "Generate high-quality images from text descriptions\n",
      "HiDream-ai\n",
      "13 days ago\n",
      "Running\n",
      "on\n",
      "CPU Upgrade\n",
      "5.5k\n",
      "MTEB Leaderboard\n",
      "ü•á\n",
      "Embedding Leaderboard\n",
      "mteb\n",
      "9 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "3.41k\n",
      "IC Light V2\n",
      "üìà\n",
      "Execute custom Python scripts from environment variables\n",
      "lllyasviel\n",
      "Oct 26, 2024\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "344\n",
      "SanaSprint\n",
      "üëÅ\n",
      "Ultra fast high quality image generation\n",
      "Efficient-Large-Model\n",
      "20 days ago\n",
      "Running\n",
      "66\n",
      "MotionShop2\n",
      "üèÉ\n",
      "Replace characters in a video with characters in photos\n",
      "3DAIGC\n",
      "11 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "4.69k\n",
      "FLUX.1 [Schnell]\n",
      "üèé\n",
      "Generate images from text prompts\n",
      "black-forest-labs\n",
      "Aug 9, 2024\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "8.15k\n",
      "FLUX.1 [dev]\n",
      "üñ•\n",
      "Generate images from text prompts\n",
      "black-forest-labs\n",
      "11 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "682\n",
      "TripoSG\n",
      "üîÆ\n",
      "Generate a 3D model from an image\n",
      "VAST-AI\n",
      "18 days ago\n",
      "Running\n",
      "345\n",
      "DeepSite Gallery\n",
      "üêã\n",
      "Browse apps made with DeepSite\n",
      "victor\n",
      "18 days ago\n",
      "Running\n",
      "on\n",
      "CPU Upgrade\n",
      "9.97k\n",
      "AI Comic Factory\n",
      "üë©\n",
      "Create your own AI comic with a single prompt\n",
      "jbilcke-hf\n",
      "Oct 15, 2024\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "1.66k\n",
      "Background Removal\n",
      "üåò\n",
      "Remove backgrounds from images\n",
      "not-lain\n",
      "Jan 2\n",
      "Running\n",
      "1.16k\n",
      "InstantCoder\n",
      "ü¶Ä\n",
      "Generate app code from ideas\n",
      "osanseviero\n",
      "Mar 25\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "48\n",
      "Chat with Bitnet-b1.58-2B-4T\n",
      "üëæ\n",
      "Chat with Microsoft's 1.58bit Bitnet model!\n",
      "suayptalha\n",
      "6 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "49\n",
      "DetailGen3D\n",
      "üëÅ\n",
      "Generate detailed 3D models from reference images and coarse models\n",
      "VAST-AI\n",
      "8 days ago\n",
      "Previous\n",
      "1\n",
      "2\n",
      "3\n",
      "...\n",
      "100\n",
      "Next\n",
      "System theme\n",
      "Company\n",
      "TOS\n",
      "Privacy\n",
      "About\n",
      "Jobs\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Pricing\n",
      "Docs\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "nari-labs/Dia-1.6B ¬∑ Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "nari-labs\n",
      "/\n",
      "Dia-1.6B\n",
      "like\n",
      "1.17k\n",
      "Follow\n",
      "Nari Labs\n",
      "554\n",
      "Text-to-Speech\n",
      "Safetensors\n",
      "English\n",
      "model_hub_mixin\n",
      "pytorch_model_hub_mixin\n",
      "arxiv:\n",
      "2305.09636\n",
      "License:\n",
      "apache-2.0\n",
      "Model card\n",
      "Files\n",
      "Files and versions\n",
      "Community\n",
      "30\n",
      "Deploy\n",
      "‚ö°Ô∏è Quickstart\n",
      "Features\n",
      "‚öôÔ∏è Usage\n",
      "As a Python Library\n",
      "üíª Hardware and Inference Speed\n",
      "ü™™ License\n",
      "‚ö†Ô∏è Disclaimer\n",
      "üî≠ TODO / Future Work\n",
      "ü§ù Contributing\n",
      "ü§ó Acknowledgements\n",
      "Dia is a 1.6B parameter text to speech model created by Nari Labs. It was pushed to the Hub using the\n",
      "PytorchModelHubMixin\n",
      "integration.\n",
      "Dia\n",
      "directly generates highly realistic dialogue from a transcript\n",
      ". You can condition the output on audio, enabling emotion and tone control. The model can also produce nonverbal communications like laughter, coughing, clearing throat, etc.\n",
      "To accelerate research, we are providing access to pretrained model checkpoints and inference code. The model weights are hosted on\n",
      "Hugging Face\n",
      ". The model only supports English generation at the moment.\n",
      "We also provide a\n",
      "demo page\n",
      "comparing our model to\n",
      "ElevenLabs Studio\n",
      "and\n",
      "Sesame CSM-1B\n",
      ".\n",
      "(Update) We have a ZeroGPU Space running! Try it now\n",
      "here\n",
      ". Thanks to the HF team for the support :)\n",
      "Join our\n",
      "discord server\n",
      "for community support and access to new features.\n",
      "Play with a larger version of Dia: generate fun conversations, remix content, and share with friends. üîÆ Join the\n",
      "waitlist\n",
      "for early access.\n",
      "‚ö°Ô∏è Quickstart\n",
      "This will open a Gradio UI that you can work on.\n",
      "git\n",
      "clone\n",
      "https://github.com/nari-labs/dia.git\n",
      "cd\n",
      "dia && uv run app.py\n",
      "or if you do not have\n",
      "uv\n",
      "pre-installed:\n",
      "git\n",
      "clone\n",
      "https://github.com/nari-labs/dia.git\n",
      "cd\n",
      "dia\n",
      "python -m venv .venv\n",
      "source\n",
      ".venv/bin/activate\n",
      "pip install uv\n",
      "uv run app.py\n",
      "Note that the model was not fine-tuned on a specific voice. Hence, you will get different voices every time you run the model.\n",
      "You can keep speaker consistency by either adding an audio prompt (a guide coming VERY soon - try it with the second example on Gradio for now), or fixing the seed.\n",
      "Features\n",
      "Generate dialogue via\n",
      "[S1]\n",
      "and\n",
      "[S2]\n",
      "tag\n",
      "Generate non-verbal like\n",
      "(laughs)\n",
      ",\n",
      "(coughs)\n",
      ", etc.\n",
      "Below verbal tags will be recognized, but might result in unexpected output.\n",
      "(laughs), (clears throat), (sighs), (gasps), (coughs), (singing), (sings), (mumbles), (beep), (groans), (sniffs), (claps), (screams), (inhales), (exhales), (applause), (burps), (humming), (sneezes), (chuckle), (whistles)\n",
      "Voice cloning. See\n",
      "example/voice_clone.py\n",
      "for more information.\n",
      "In the Hugging Face space, you can upload the audio you want to clone and place its transcript before your script. Make sure the transcript follows the required format. The model will then output only the content of your script.\n",
      "‚öôÔ∏è Usage\n",
      "As a Python Library\n",
      "import\n",
      "soundfile\n",
      "as\n",
      "sf\n",
      "from\n",
      "dia.model\n",
      "import\n",
      "Dia\n",
      "\n",
      "\n",
      "model = Dia.from_pretrained(\n",
      "\"nari-labs/Dia-1.6B\"\n",
      ")\n",
      "\n",
      "text =\n",
      "\"[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face.\"\n",
      "output = model.generate(text)\n",
      "\n",
      "sf.write(\n",
      "\"simple.mp3\"\n",
      ", output,\n",
      "44100\n",
      ")\n",
      "A pypi package and a working CLI tool will be available soon.\n",
      "üíª Hardware and Inference Speed\n",
      "Dia has been tested on only GPUs (pytorch 2.0+, CUDA 12.6). CPU support is to be added soon.\n",
      "The initial run will take longer as the Descript Audio Codec also needs to be downloaded.\n",
      "On enterprise GPUs, Dia can generate audio in real-time. On older GPUs, inference time will be slower.\n",
      "For reference, on a A4000 GPU, Dia roughly generates 40 tokens/s (86 tokens equals 1 second of audio).\n",
      "torch.compile\n",
      "will increase speeds for supported GPUs.\n",
      "The full version of Dia requires around 10GB of VRAM to run. We will be adding a quantized version in the future.\n",
      "If you don't have hardware available or if you want to play with bigger versions of our models, join the waitlist\n",
      "here\n",
      ".\n",
      "ü™™ License\n",
      "This project is licensed under the Apache License 2.0 - see the\n",
      "LICENSE\n",
      "file for details.\n",
      "‚ö†Ô∏è Disclaimer\n",
      "This project offers a high-fidelity speech generation model intended for research and educational use. The following uses are\n",
      "strictly forbidden\n",
      ":\n",
      "Identity Misuse\n",
      ": Do not produce audio resembling real individuals without permission.\n",
      "Deceptive Content\n",
      ": Do not use this model to generate misleading content (e.g. fake news)\n",
      "Illegal or Malicious Use\n",
      ": Do not use this model for activities that are illegal or intended to cause harm.\n",
      "By using this model, you agree to uphold relevant legal standards and ethical responsibilities. We\n",
      "are not responsible\n",
      "for any misuse and firmly oppose any unethical usage of this technology.\n",
      "üî≠ TODO / Future Work\n",
      "Docker support.\n",
      "Optimize inference speed.\n",
      "Add quantization for memory efficiency.\n",
      "ü§ù Contributing\n",
      "We are a tiny team of 1 full-time and 1 part-time research-engineers. We are extra-welcome to any contributions!\n",
      "Join our\n",
      "Discord Server\n",
      "for discussions.\n",
      "ü§ó Acknowledgements\n",
      "We thank the\n",
      "Google TPU Research Cloud program\n",
      "for providing computation resources.\n",
      "Our work was heavily inspired by\n",
      "SoundStorm\n",
      ",\n",
      "Parakeet\n",
      ", and\n",
      "Descript Audio Codec\n",
      ".\n",
      "HuggingFace for providing the ZeroGPU Grant.\n",
      "\"Nari\" is a pure Korean word for lily.\n",
      "We thank Jason Y. for providing help with data filtering.\n",
      "Downloads last month\n",
      "63,338\n",
      "Safetensors\n",
      "Model size\n",
      "1.61B params\n",
      "Tensor type\n",
      "F32\n",
      "¬∑\n",
      "Files info\n",
      "Inference Providers\n",
      "NEW\n",
      "fal\n",
      "Text-to-Speech\n",
      "Examples\n",
      "Compute\n",
      "View Code\n",
      "Maximize\n",
      "Model tree for\n",
      "nari-labs/Dia-1.6B\n",
      "Finetunes\n",
      "4 models\n",
      "Quantizations\n",
      "3 models\n",
      "Spaces using\n",
      "nari-labs/Dia-1.6B\n",
      "76\n",
      "nari-labs/Dia-1.6B\n",
      "üöÄ\n",
      "mrfakename/dia-1.6b\n",
      "üëØ\n",
      "Fizzarolli/Dia-1.6B\n",
      "üëØ\n",
      "abidlabs/Dia-1.6B\n",
      "üëØ\n",
      "Nymbo/Dia-1.6B\n",
      "üëØ\n",
      "Gyaneshere/Transcript-to-Speech\n",
      "üèÜ\n",
      "manfromexistence-reacts/nari-labs-Dia-1.6B\n",
      "üê¢\n",
      "broadfield-dev/Dia-Demo-Light-CPU\n",
      "üöÄ\n",
      "charlie0simmon/dia-1.6b\n",
      "üî•\n",
      "DFZR/nari-labs-Dia-1.6B\n",
      "‚ö°\n",
      "smjack/nari-labs-Dia-1.6B\n",
      "üìâ\n",
      "storyjacker/nari-labs-Dia-1.6B\n",
      "+ 71 Spaces\n",
      "+ 64 Spaces\n",
      "System theme\n",
      "Company\n",
      "TOS\n",
      "Privacy\n",
      "About\n",
      "Jobs\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Pricing\n",
      "Docs\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "sand-ai/MAGI-1 ¬∑ Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "sand-ai\n",
      "/\n",
      "MAGI-1\n",
      "like\n",
      "394\n",
      "Follow\n",
      "Sand AI\n",
      "180\n",
      "Image-to-Video\n",
      "Diffusers\n",
      "Safetensors\n",
      "English\n",
      "MAGI-1\n",
      "License:\n",
      "apache-2.0\n",
      "Model card\n",
      "Files\n",
      "Files and versions\n",
      "Community\n",
      "17\n",
      "MAGI-1: Autoregressive Video Generation at Scale\n",
      "üî•üî•üî• Latest News\n",
      "1. About\n",
      "2. Model Summary\n",
      "Transformer-based VAE\n",
      "Auto-Regressive Denoising Algorithm\n",
      "Diffusion Model Architecture\n",
      "Distillation Algorithm\n",
      "3. Model Zoo\n",
      "4. Evaluation\n",
      "In-house Human Evaluation\n",
      "Physical Evaluation\n",
      "5. How to run\n",
      "Environment Preparation\n",
      "Inference Command\n",
      "Some Useful Configs (for config.json)\n",
      "6. License\n",
      "7. Citation\n",
      "8. Contact\n",
      "MAGI-1: Autoregressive Video Generation at Scale\n",
      "This repository contains the code for the MAGI-1 model, pre-trained weights and inference code. You can find more information on our\n",
      "technical report\n",
      "or directly create magic with MAGI-1\n",
      "here\n",
      ". üöÄ‚ú®\n",
      "üî•üî•üî• Latest News\n",
      "Apr 21, 2025: MAGI-1 is here üéâ. We've released the model weights and inference code ‚Äî check it out!\n",
      "1. About\n",
      "We present MAGI-1, a world model that generates videos by\n",
      "autoregressively\n",
      "predicting a sequence of video chunks, defined as fixed-length segments of consecutive frames. Trained to denoise per-chunk noise that increases monotonically over time, MAGI-1 enables causal temporal modeling and naturally supports streaming generation. It achieves strong performance on image-to-video (I2V) tasks conditioned on text instructions, providing high temporal consistency and scalability, which are made possible by several algorithmic innovations and a dedicated infrastructure stack. MAGI-1 further supports controllable generation via chunk-wise prompting, enabling smooth scene transitions, long-horizon synthesis, and fine-grained text-driven control. We believe MAGI-1 offers a promising direction for unifying high-fidelity video generation with flexible instruction control and real-time deployment.\n",
      "2. Model Summary\n",
      "Transformer-based VAE\n",
      "Variational autoencoder (VAE) with transformer-based architecture, 8x spatial and 4x temporal compression.\n",
      "Fastest average decoding time and highly competitive reconstruction quality\n",
      "Auto-Regressive Denoising Algorithm\n",
      "MAGI-1 is an autoregressive denoising video generation model generating videos chunk-by-chunk instead of as a whole. Each chunk (24 frames) is denoised holistically, and the generation of the next chunk begins as soon as the current one reaches a certain level of denoising. This pipeline design enables concurrent processing of up to four chunks for efficient video generation.\n",
      "Diffusion Model Architecture\n",
      "MAGI-1 is built upon the Diffusion Transformer, incorporating several key innovations to enhance training efficiency and stability at scale. These advancements include Block-Causal Attention, Parallel Attention Block, QK-Norm and GQA, Sandwich Normalization in FFN, SwiGLU, and Softcap Modulation. For more details, please refer to the\n",
      "technical report.\n",
      "Distillation Algorithm\n",
      "We adopt a shortcut distillation approach that trains a single velocity-based model to support variable inference budgets. By enforcing a self-consistency constraint‚Äîequating one large step with two smaller steps‚Äîthe model learns to approximate flow-matching trajectories across multiple step sizes. During training, step sizes are cyclically sampled from {64, 32, 16, 8}, and classifier-free guidance distillation is incorporated to preserve conditional alignment. This enables efficient inference with minimal loss in fidelity.\n",
      "3. Model Zoo\n",
      "We provide the pre-trained weights for MAGI-1, including the 24B and 4.5B models, as well as the corresponding distill and distill+quant models. The model weight links are shown in the table.\n",
      "Model\n",
      "Link\n",
      "Recommend Machine\n",
      "T5\n",
      "T5\n",
      "-\n",
      "MAGI-1-VAE\n",
      "MAGI-1-VAE\n",
      "-\n",
      "MAGI-1-24B\n",
      "MAGI-1-24B\n",
      "H100/H800 * 8\n",
      "MAGI-1-24B-distill\n",
      "MAGI-1-24B-distill\n",
      "H100/H800 * 8\n",
      "MAGI-1-24B-distill+fp8_quant\n",
      "MAGI-1-24B-distill+quant\n",
      "H100/H800 * 4 or RTX 4090 * 8\n",
      "MAGI-1-4.5B\n",
      "MAGI-1-4.5B\n",
      "RTX 4090 * 1\n",
      "4. Evaluation\n",
      "In-house Human Evaluation\n",
      "MAGI-1 achieves state-of-the-art performance among open-source models (surpassing Wan-2.1 and significantly outperforming Hailuo and HunyuanVideo), particularly excelling in instruction following and motion quality, positioning it as a strong potential competitor to closed-source commercial models such as Kling.\n",
      "Physical Evaluation\n",
      "Thanks to the natural advantages of autoregressive architecture, Magi achieves far superior precision in predicting physical behavior through video continuation‚Äîsignificantly outperforming all existing models.\n",
      "Model\n",
      "Phys. IQ Score ‚Üë\n",
      "Spatial IoU ‚Üë\n",
      "Spatio Temporal ‚Üë\n",
      "Weighted Spatial IoU ‚Üë\n",
      "MSE ‚Üì\n",
      "V2V Models\n",
      "Magi (V2V)\n",
      "56.02\n",
      "0.367\n",
      "0.270\n",
      "0.304\n",
      "0.005\n",
      "VideoPoet (V2V)\n",
      "29.50\n",
      "0.204\n",
      "0.164\n",
      "0.137\n",
      "0.010\n",
      "I2V Models\n",
      "Magi (I2V)\n",
      "30.23\n",
      "0.203\n",
      "0.151\n",
      "0.154\n",
      "0.012\n",
      "Kling1.6 (I2V)\n",
      "23.64\n",
      "0.197\n",
      "0.086\n",
      "0.144\n",
      "0.025\n",
      "VideoPoet (I2V)\n",
      "20.30\n",
      "0.141\n",
      "0.126\n",
      "0.087\n",
      "0.012\n",
      "Gen 3 (I2V)\n",
      "22.80\n",
      "0.201\n",
      "0.115\n",
      "0.116\n",
      "0.015\n",
      "Wan2.1 (I2V)\n",
      "20.89\n",
      "0.153\n",
      "0.100\n",
      "0.112\n",
      "0.023\n",
      "Sora (I2V)\n",
      "10.00\n",
      "0.138\n",
      "0.047\n",
      "0.063\n",
      "0.030\n",
      "GroundTruth\n",
      "100.0\n",
      "0.678\n",
      "0.535\n",
      "0.577\n",
      "0.002\n",
      "5. How to run\n",
      "Environment Preparation\n",
      "We provide two ways to run MAGI-1, with the Docker environment being the recommended option.\n",
      "Run with Docker Environment (Recommend)\n",
      "docker pull sandai/magi:latest\n",
      "\n",
      "docker run -it --gpus all --privileged --shm-size=32g --name magi --net=host --ipc=host --\n",
      "ulimit\n",
      "memlock=-1 --\n",
      "ulimit\n",
      "stack=6710886 sandai/magi:latest /bin/bash\n",
      "Run with Source Code\n",
      "# Create a new environment\n",
      "conda create -n magi python==3.10.12\n",
      "# Install pytorch\n",
      "conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=12.4 -c pytorch -c nvidia\n",
      "# Install other dependencies\n",
      "pip install -r requirements.txt\n",
      "# Install ffmpeg\n",
      "conda install -c conda-forge ffmpeg=4.4\n",
      "# Install MagiAttention, for more information, please refer to https://github.com/SandAI-org/MagiAttention#\n",
      "git\n",
      "clone\n",
      "git@github.com:SandAI-org/MagiAttention.git\n",
      "cd\n",
      "MagiAttention\n",
      "git submodule update --init --recursive\n",
      "pip install --no-build-isolation .\n",
      "Inference Command\n",
      "To run the\n",
      "MagiPipeline\n",
      ", you can control the input and output by modifying the parameters in the\n",
      "example/24B/run.sh\n",
      "or\n",
      "example/4.5B/run.sh\n",
      "script. Below is an explanation of the key parameters:\n",
      "Parameter Descriptions\n",
      "--config_file\n",
      ": Specifies the path to the configuration file, which contains model configuration parameters, e.g.,\n",
      "example/24B/24B_config.json\n",
      ".\n",
      "--mode\n",
      ": Specifies the mode of operation. Available options are:\n",
      "t2v\n",
      ": Text to Video\n",
      "i2v\n",
      ": Image to Video\n",
      "v2v\n",
      ": Video to Video\n",
      "--prompt\n",
      ": The text prompt used for video generation, e.g.,\n",
      "\"Good Boy\"\n",
      ".\n",
      "--image_path\n",
      ": Path to the image file, used only in\n",
      "i2v\n",
      "mode.\n",
      "--prefix_video_path\n",
      ": Path to the prefix video file, used only in\n",
      "v2v\n",
      "mode.\n",
      "--output_path\n",
      ": Path where the generated video file will be saved.\n",
      "Bash Script\n",
      "#!/bin/bash\n",
      "# Run 24B MAGI-1 model\n",
      "bash example/24B/run.sh\n",
      "# Run 4.5B MAGI-1 model\n",
      "bash example/4.5B/run.sh\n",
      "Customizing Parameters\n",
      "You can modify the parameters in\n",
      "run.sh\n",
      "as needed. For example:\n",
      "To use the Image to Video mode (\n",
      "i2v\n",
      "), set\n",
      "--mode\n",
      "to\n",
      "i2v\n",
      "and provide\n",
      "--image_path\n",
      ":\n",
      "--mode i2v \\\n",
      "--image_path example/assets/image.jpeg \\\n",
      "To use the Video to Video mode (\n",
      "v2v\n",
      "), set\n",
      "--mode\n",
      "to\n",
      "v2v\n",
      "and provide\n",
      "--prefix_video_path\n",
      ":\n",
      "--mode v2v \\\n",
      "--prefix_video_path example/assets/prefix_video.mp4 \\\n",
      "By adjusting these parameters, you can flexibly control the input and output to meet different requirements.\n",
      "Some Useful Configs (for config.json)\n",
      "Config\n",
      "Help\n",
      "seed\n",
      "Random seed used for video generation\n",
      "video_size_h\n",
      "Height of the video\n",
      "video_size_w\n",
      "Width of the video\n",
      "num_frames\n",
      "Controls the duration of generated video\n",
      "fps\n",
      "Frames per second, 4 video frames correspond to 1 latent_frame\n",
      "cfg_number\n",
      "Base model uses cfg_number==2, distill and quant model uses cfg_number=1\n",
      "load\n",
      "Directory containing a model checkpoint.\n",
      "t5_pretrained\n",
      "Path to load pretrained T5 model\n",
      "vae_pretrained\n",
      "Path to load pretrained VAE model\n",
      "6. License\n",
      "This project is licensed under the Apache License 2.0 - see the\n",
      "LICENSE\n",
      "file for details.\n",
      "7. Citation\n",
      "If you find our code or model useful in your research, please cite:\n",
      "@misc{magi1,\n",
      "      title={MAGI-1: Autoregressive Video Generation at Scale},\n",
      "      author={Sand-AI},\n",
      "      year={2025},\n",
      "      url={https://static.magi.world/static/files/MAGI_1.pdf},\n",
      "}\n",
      "8. Contact\n",
      "If you have any questions, please feel free to raise an issue or contact us at\n",
      "support@sand.ai\n",
      ".\n",
      "Downloads last month\n",
      "-\n",
      "Downloads are not tracked for this model.\n",
      "How to track\n",
      "Inference Providers\n",
      "NEW\n",
      "Image-to-Video\n",
      "This model isn't deployed by any Inference Provider.\n",
      "üôã\n",
      "21\n",
      "Ask for provider support\n",
      "System theme\n",
      "Company\n",
      "TOS\n",
      "Privacy\n",
      "About\n",
      "Jobs\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Pricing\n",
      "Docs\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "404 ‚Äì Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "404\n",
      "Sorry, we can't find the page you are looking for.\n",
      "System theme\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Tasks\n",
      "Inference Endpoints\n",
      "HuggingChat\n",
      "Company\n",
      "About\n",
      "Brand assets\n",
      "Terms of service\n",
      "Privacy\n",
      "Jobs\n",
      "Press\n",
      "Resources\n",
      "Learn\n",
      "Documentation\n",
      "Blog\n",
      "Forum\n",
      "Service Status\n",
      "Social\n",
      "GitHub\n",
      "Twitter\n",
      "LinkedIn\n",
      "Discord\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "ostris/Flex.2-preview ¬∑ Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "ostris\n",
      "/\n",
      "Flex.2-preview\n",
      "like\n",
      "203\n",
      "Text-to-Image\n",
      "Diffusers\n",
      "Safetensors\n",
      "FluxPipeline\n",
      "License:\n",
      "apache-2.0\n",
      "Model card\n",
      "Files\n",
      "Files and versions\n",
      "Community\n",
      "9\n",
      "Deploy\n",
      "Use this model\n",
      "Flex.2-preview\n",
      "Features\n",
      "About\n",
      "Usage\n",
      "ComfyUI\n",
      "Diffusers\n",
      "Limitations\n",
      "Fine Tuning\n",
      "Feedback\n",
      "Control Implementation\n",
      "Support My Work\n",
      "Current Sponsors\n",
      "Flex.2-preview\n",
      "Open Source 8B parameter Text to Image Diffusion Model with universal control and inpainting support built in. Early access preview release. The next version of\n",
      "Flex.1-alpha\n",
      "Features\n",
      "8 billion parameters\n",
      "Guidance embedder (2x as fast to generate)\n",
      "Built in inpainting\n",
      "Universal control input (line, pose, depth)\n",
      "Fine tunable\n",
      "OSI compliant license (Apache 2.0)\n",
      "512 token length input\n",
      "16 channel latent space\n",
      "Made by the community, for the community\n",
      "About\n",
      "Flex.2 is currently the most flexable text to image diffusion model released, making it truely live up to its name. It has taken a lot to get here:\n",
      "Flux.1 Schnell\n",
      "->\n",
      "OpenFlux.1\n",
      "->\n",
      "Flex.1-alpha\n",
      "->\n",
      "Flex.2-preview\n",
      ".\n",
      "Numerous improvements have been made with every step, but Flex.2 is the biggest step so far, and best of all, it was just trained by a some guy who relies 100% on community support to make a living and fund the outrageous compute cost needed to keep training models like this. Speaking of which, and since you brought it up, not me, I am always in need of support. Everything I create is free and open, with permissive licenses. So if you find my work beneficial, or use it in a commercial setting, please consider contributing to my continued desire to live and develop more open source projects and models. Visit the\n",
      "Support\n",
      "section below to find out how you can help out and see the awesome people who already do.\n",
      "Flex.2 preview is an early release to get feedback on the new features and to encourage experimentation and tooling. I would love to hear suggestions as well as be made aware of weak points \n",
      "so I can address them while training continues. Flex.2 is a continuation of\n",
      "Flex.1-alpha\n",
      ", with a lot of goodies thrown in. The most important \n",
      "new features and improvements over\n",
      "Flex.1-alpha\n",
      "are:\n",
      "Inpainting\n",
      ": Flex.2 as built in inpainting support trained into the base model.\n",
      "Universal Control\n",
      ": It has a universal control input that has been trained to accept pose, line, and depth inputs.\n",
      "I wanted to put all the tools I use and love straight into the base model so one model is all you need to empower creativity far beyond what a simple text to image model could ever do on its own.\n",
      "Usage\n",
      "ComfyUI\n",
      "Flex.2 is supported in ComfyUI with the help of the Flex2 Conditioner node found in\n",
      "ComfyUI-FlexTools\n",
      ". I also recommend using\n",
      "comfyui_controlnet_aux\n",
      "to generate the control images (pose and depth) This conditioning node handles all of the controls and inpainting conditioning for you, but is also needed for normal T2I generation. So grab and install those first.\n",
      "For the model, follow the directions for flux for the vae and text encoder models from the\n",
      "ComfyUI examples\n",
      ". You dont need the diffusion_model. If you have Flux in Comfy, you can probably skip this step. Then download\n",
      "Flex.2-preview.safetensors\n",
      "to\n",
      "ComfyUI/models/diffusion_models/Flex.2-preview.safetensors\n",
      "and restart Comfy. You can use the workflow in this image as a starting point example for controls and inpainting.\n",
      "Diffusers\n",
      "For diffusers, you can use\n",
      "AutoPipelineForText2Image\n",
      ". This will load the model with the pipeline located\n",
      "here\n",
      ". We will run this example using the\n",
      "diffusers\n",
      "library.\n",
      "First install the requirements.\n",
      "pip install --upgrade torch accelerate transformers diffusers\n",
      "Then you can load the model and use it like this:\n",
      "import\n",
      "torch\n",
      "from\n",
      "diffusers\n",
      "import\n",
      "AutoPipelineForText2Image\n",
      "from\n",
      "diffusers.utils\n",
      "import\n",
      "load_image\n",
      "\n",
      "name_or_path =\n",
      "\"ostris/Flex.2-preview\"\n",
      "inpaint_image = load_image(\n",
      "\"https://ostris.com/wp-content/uploads/2025/04/dog.jpg\"\n",
      ")\n",
      "inpaint_mask = load_image(\n",
      "\"https://ostris.com/wp-content/uploads/2025/04/dog_mask.jpg\"\n",
      ")\n",
      "control_image = load_image(\n",
      "\"https://ostris.com/wp-content/uploads/2025/04/dog_depth.jpg\"\n",
      ")\n",
      "\n",
      "dtype = torch.bfloat16\n",
      "\n",
      "pipe = AutoPipelineForText2Image.from_pretrained(\n",
      "    name_or_path,\n",
      "    custom_pipeline=name_or_path,\n",
      "    torch_dtype=dtype\n",
      ").to(\n",
      "\"cuda\"\n",
      ")\n",
      "\n",
      "image = pipe(\n",
      "    prompt=\n",
      "\"A white friendly robotic dog sitting on a bench\"\n",
      ",\n",
      "    inpaint_image=inpaint_image,\n",
      "    inpaint_mask=inpaint_mask,\n",
      "    control_image=control_image,\n",
      "    control_strength=\n",
      "0.5\n",
      ",\n",
      "    control_stop=\n",
      "0.33\n",
      ",\n",
      "    height=\n",
      "1024\n",
      ",\n",
      "    width=\n",
      "1024\n",
      ",\n",
      "    guidance_scale=\n",
      "3.5\n",
      ",\n",
      "    num_inference_steps=\n",
      "50\n",
      ",\n",
      "    generator=torch.Generator(\n",
      "\"cpu\"\n",
      ").manual_seed(\n",
      "42\n",
      ")\n",
      ").images[\n",
      "0\n",
      "]\n",
      "image.save(\n",
      "f\"robot_dog.png\"\n",
      ")\n",
      "For consumer cards < 24GB, you can use torchao to\n",
      "pip install --upgrade torchao\n",
      "Then you can load the model and use it like this:\n",
      "import\n",
      "torch\n",
      "from\n",
      "diffusers\n",
      "import\n",
      "AutoPipelineForText2Image\n",
      "from\n",
      "diffusers.utils\n",
      "import\n",
      "load_image\n",
      "from\n",
      "transformers\n",
      "import\n",
      "T5EncoderModel, TorchAoConfig\n",
      "from\n",
      "diffusers\n",
      "import\n",
      "FluxTransformer2DModel\n",
      "\n",
      "name_or_path =\n",
      "\"ostris/Flex.2-preview\"\n",
      "inpaint_image = load_image(\n",
      "\"https://ostris.com/wp-content/uploads/2025/04/dog.jpg\"\n",
      ")\n",
      "inpaint_mask = load_image(\n",
      "\"https://ostris.com/wp-content/uploads/2025/04/dog_mask.jpg\"\n",
      ")\n",
      "control_image = load_image(\n",
      "\"https://ostris.com/wp-content/uploads/2025/04/dog_depth.jpg\"\n",
      ")\n",
      "\n",
      "dtype = torch.bfloat16\n",
      "\n",
      "quant_config = TorchAoConfig(\n",
      "\"int8_weight_only\"\n",
      ")\n",
      "\n",
      "text_encoder_2 = T5EncoderModel.from_pretrained(\n",
      "    name_or_path, subfolder=\n",
      "\"text_encoder_2\"\n",
      ", torch_dtype=dtype, quantization_config=quant_config\n",
      ").to(\n",
      "\"cuda\"\n",
      ")\n",
      "\n",
      "transformer = FluxTransformer2DModel.from_pretrained(\n",
      "    name_or_path, subfolder=\n",
      "\"transformer\"\n",
      ", torch_dtype=dtype, quantization_config=quant_config\n",
      ").to(\n",
      "\"cuda\"\n",
      ")\n",
      "\n",
      "\n",
      "pipe = AutoPipelineForText2Image.from_pretrained(\n",
      "    name_or_path,\n",
      "    transformer=transformer,\n",
      "    text_encoder_2=text_encoder_2,\n",
      "    custom_pipeline=name_or_path,\n",
      "    torch_dtype=dtype\n",
      ").to(\n",
      "\"cuda\"\n",
      ")\n",
      "\n",
      "image = pipe(\n",
      "    prompt=\n",
      "\"A white friendly robotic dog sitting on a bench\"\n",
      ",\n",
      "    inpaint_image=inpaint_image,\n",
      "    inpaint_mask=inpaint_mask,\n",
      "    control_image=control_image,\n",
      "    control_strength=\n",
      "0.5\n",
      ",\n",
      "    control_stop=\n",
      "0.33\n",
      ",\n",
      "    height=\n",
      "1024\n",
      ",\n",
      "    width=\n",
      "1024\n",
      ",\n",
      "    guidance_scale=\n",
      "3.5\n",
      ",\n",
      "    num_inference_steps=\n",
      "50\n",
      ",\n",
      "    generator=torch.Generator(\n",
      "\"cpu\"\n",
      ").manual_seed(\n",
      "42\n",
      ")\n",
      ").images[\n",
      "0\n",
      "]\n",
      "image.save(\n",
      "f\"robot_dog.png\"\n",
      ")\n",
      "Limitations\n",
      "This model is still experimental and under active development. The training for the controls and inpainting was pretty aggressive, and the model currently struggles with a few\n",
      "things such an anatomy and text. The inpainting is also still being worked on. This is just\n",
      "a preview release. I am working on improving these limitations with each new training run.\n",
      "Fine Tuning\n",
      "Flex.2 is designed to be fine tunable, though the best pratice for doing so is still highly experimental as a model like\n",
      "this has not really existed before. You can train traditional LoRAs directly on a model that can do controls and inpainting. Flex.1-alpha LoRAs also tento to work well with it. Day 1 support is already live for LoRA training with\n",
      "AI-Toolkit\n",
      "with built in functionality to automatically generate the controls and inpainting inputs for your existing datasets. Check out the example config file and read through the comments \n",
      "to see how it works\n",
      "train_lora_flex2_24gb.yaml\n",
      "You can also make your own controls and teach the model to use them by training a simple LoRA in no more time than it would take to train a normal LoRA.\n",
      "Feedback\n",
      "I would love to hear feedback on the model and on things that need improvement. Please\n",
      "Join my Discord\n",
      ". I am also open to suggestions for new features and improvements.\n",
      "Control Implementation\n",
      "The technical stuff for those implementing tools and inference for it. General users can skip this part.\n",
      "The control and inpainting implementation work similar to Flux controls, though the inpainting is slightly different. For Flux/Flex has a \n",
      "16 channel latent space that is packed at half the size into a 64 channel latent. The controls can be added before or after packing, but for suimplicity,\n",
      "I will describe them on a 16channel unpacked latent.\n",
      "(16ch = Noisy latent) + (16ch = VAE encoded inpainting image) + (1ch = inpainting mask) + (16ch = control input) = 49 channels\n",
      "16ch Noisy Latent\n",
      "- This works as it has previously and is the normal input for a Flex/Flux model.\n",
      "16ch VAE encoded inpainting image\n",
      "- The inpainting image is encoded using the VAE without any masking. Once in the latent space. The desirec inpait section is zeroed out by multiplying by the inverted mask.\n",
      "To disable this input, simply feed 0s for these channels to indicate the entire image is inpainted.\n",
      "1ch Inpainting mask\n",
      "- The inpainting mask is a single channel image that indicates which pixels are inpainted. The mask is 0 for pixels in the inpainting image to keep and 1 for the area to inpaint. The mask can just be scaled down directly from the pixel space.\n",
      "To disable this input, along with inpainting, feed 1s for this channel to indicate the entire image is inpainted.\n",
      "16ch Control input\n",
      "- A VAE encoded control input. The model was trained on pose, line, and depth inputs. To disable control, simply feed 0s for these channels to indicate no control input.\n",
      "For normal T2I generation without any controls or inputs, you simpley need.\n",
      "model_input = torch.cat([\n",
      "    latent,\n",
      "# Noisy latent\n",
      "torch.zeros_like(latent),\n",
      "# VAE encoded inpainting image\n",
      "torch.ones_like(latent)[:,\n",
      "0\n",
      ":\n",
      "1\n",
      ", :, :],\n",
      "# 0 - 1 inpaint mask\n",
      "torch.zeros_like(latent)\n",
      "# VAE encoded control input\n",
      "], dim=\n",
      "1\n",
      ")\n",
      "Support My Work\n",
      "If you enjoy my projects or use them commercially, please consider sponsoring me. Every bit helps! üíñ\n",
      "Sponsor on GitHub\n",
      "|\n",
      "Support on Patreon\n",
      "|\n",
      "Donate on PayPal\n",
      "Current Sponsors\n",
      "All of these people / organizations are the ones who selflessly make this project possible. Thank you!!\n",
      "Last updated: 2025-04-23 18:04 UTC\n",
      "Downloads last month\n",
      "3,846\n",
      "Inference Providers\n",
      "NEW\n",
      "Text-to-Image\n",
      "This model isn't deployed by any Inference Provider.\n",
      "üôã\n",
      "1\n",
      "Ask for provider support\n",
      "Model tree for\n",
      "ostris/Flex.2-preview\n",
      "Quantizations\n",
      "1 model\n",
      "Space using\n",
      "ostris/Flex.2-preview\n",
      "1\n",
      "üëÅ\n",
      "suayptalha/Flex.2-preview\n",
      "System theme\n",
      "Company\n",
      "TOS\n",
      "Privacy\n",
      "About\n",
      "Jobs\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Pricing\n",
      "Docs\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "HiDream-ai/HiDream-I1-Full ¬∑ Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "HiDream-ai\n",
      "/\n",
      "HiDream-I1-Full\n",
      "like\n",
      "747\n",
      "Follow\n",
      "HiDream.ai\n",
      "608\n",
      "Text-to-Image\n",
      "Diffusers\n",
      "Safetensors\n",
      "English\n",
      "HiDreamImagePipeline\n",
      "image-generation\n",
      "HiDream.ai\n",
      "License:\n",
      "mit\n",
      "Model card\n",
      "Files\n",
      "Files and versions\n",
      "Community\n",
      "37\n",
      "Deploy\n",
      "Use this model\n",
      "Key Features\n",
      "Quick Start\n",
      "Gradio Demo\n",
      "Evaluation Metrics\n",
      "DPG-Bench\n",
      "GenEval\n",
      "HPSv2.1 benchmark\n",
      "License Agreement\n",
      "Acknowledgements\n",
      "HiDream-I1\n",
      "is a new open-source image generative foundation model with 17B parameters that achieves state-of-the-art image generation quality within seconds.\n",
      "For more features and to experience the full capabilities of our product, please visit\n",
      "https://vivago.ai/\n",
      ".\n",
      "Key Features\n",
      "‚ú®\n",
      "Superior Image Quality\n",
      "- Produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. Achieves state-of-the-art HPS v2.1 score, which aligns with human preferences.\n",
      "üéØ\n",
      "Best-in-Class Prompt Following\n",
      "- Achieves industry-leading scores on GenEval and DPG benchmarks, outperforming all other open-source models.\n",
      "üîì\n",
      "Open Source\n",
      "- Released under the MIT license to foster scientific advancement and enable creative innovation.\n",
      "üíº\n",
      "Commercial-Friendly\n",
      "- Generated images can be freely used for personal projects, scientific research, and commercial applications.\n",
      "Quick Start\n",
      "Please make sure you have installed\n",
      "Flash Attention\n",
      ". We recommend CUDA version 12.4 for the manual installation.\n",
      "pip install -r requirements.txt\n",
      "Clone the GitHub repo:\n",
      "git clone https://github.com/HiDream-ai/HiDream-I1\n",
      "Then you can run the inference scripts to generate images:\n",
      "# For full model inference\n",
      "python ./inference.py --model_type full\n",
      "# For distilled dev model inference\n",
      "python ./inference.py --model_type dev\n",
      "# For distilled fast model inference\n",
      "python ./inference.py --model_type fast\n",
      "Note:\n",
      "The inference script will automatically download\n",
      "meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "model files. If you encounter network issues, you can download these files ahead of time and place them in the appropriate cache directory to avoid download failures during inference.\n",
      "Gradio Demo\n",
      "We also provide a Gradio demo for interactive image generation. You can run the demo with:\n",
      "python gradio_demo.py\n",
      "Evaluation Metrics\n",
      "DPG-Bench\n",
      "Model\n",
      "Overall\n",
      "Global\n",
      "Entity\n",
      "Attribute\n",
      "Relation\n",
      "Other\n",
      "PixArt-alpha\n",
      "71.11\n",
      "74.97\n",
      "79.32\n",
      "78.60\n",
      "82.57\n",
      "76.96\n",
      "SDXL\n",
      "74.65\n",
      "83.27\n",
      "82.43\n",
      "80.91\n",
      "86.76\n",
      "80.41\n",
      "DALL-E 3\n",
      "83.50\n",
      "90.97\n",
      "89.61\n",
      "88.39\n",
      "90.58\n",
      "89.83\n",
      "Flux.1-dev\n",
      "83.79\n",
      "85.80\n",
      "86.79\n",
      "89.98\n",
      "90.04\n",
      "89.90\n",
      "SD3-Medium\n",
      "84.08\n",
      "87.90\n",
      "91.01\n",
      "88.83\n",
      "80.70\n",
      "88.68\n",
      "Janus-Pro-7B\n",
      "84.19\n",
      "86.90\n",
      "88.90\n",
      "89.40\n",
      "89.32\n",
      "89.48\n",
      "CogView4-6B\n",
      "85.13\n",
      "83.85\n",
      "90.35\n",
      "91.17\n",
      "91.14\n",
      "87.29\n",
      "HiDream-I1\n",
      "85.89\n",
      "76.44\n",
      "90.22\n",
      "89.48\n",
      "93.74\n",
      "91.83\n",
      "GenEval\n",
      "Model\n",
      "Overall\n",
      "Single Obj.\n",
      "Two Obj.\n",
      "Counting\n",
      "Colors\n",
      "Position\n",
      "Color attribution\n",
      "SDXL\n",
      "0.55\n",
      "0.98\n",
      "0.74\n",
      "0.39\n",
      "0.85\n",
      "0.15\n",
      "0.23\n",
      "PixArt-alpha\n",
      "0.48\n",
      "0.98\n",
      "0.50\n",
      "0.44\n",
      "0.80\n",
      "0.08\n",
      "0.07\n",
      "Flux.1-dev\n",
      "0.66\n",
      "0.98\n",
      "0.79\n",
      "0.73\n",
      "0.77\n",
      "0.22\n",
      "0.45\n",
      "DALL-E 3\n",
      "0.67\n",
      "0.96\n",
      "0.87\n",
      "0.47\n",
      "0.83\n",
      "0.43\n",
      "0.45\n",
      "CogView4-6B\n",
      "0.73\n",
      "0.99\n",
      "0.86\n",
      "0.66\n",
      "0.79\n",
      "0.48\n",
      "0.58\n",
      "SD3-Medium\n",
      "0.74\n",
      "0.99\n",
      "0.94\n",
      "0.72\n",
      "0.89\n",
      "0.33\n",
      "0.60\n",
      "Janus-Pro-7B\n",
      "0.80\n",
      "0.99\n",
      "0.89\n",
      "0.59\n",
      "0.90\n",
      "0.79\n",
      "0.66\n",
      "HiDream-I1\n",
      "0.83\n",
      "1.00\n",
      "0.98\n",
      "0.79\n",
      "0.91\n",
      "0.60\n",
      "0.72\n",
      "HPSv2.1 benchmark\n",
      "Model\n",
      "Averaged\n",
      "Animation\n",
      "Concept-art\n",
      "Painting\n",
      "Photo\n",
      "Stable Diffusion v2.0\n",
      "26.38\n",
      "27.09\n",
      "26.02\n",
      "25.68\n",
      "26.73\n",
      "Midjourney V6\n",
      "30.29\n",
      "32.02\n",
      "30.29\n",
      "29.74\n",
      "29.10\n",
      "SDXL\n",
      "30.64\n",
      "32.84\n",
      "31.36\n",
      "30.86\n",
      "27.48\n",
      "Dall-E3\n",
      "31.44\n",
      "32.39\n",
      "31.09\n",
      "31.18\n",
      "31.09\n",
      "SD3\n",
      "31.53\n",
      "32.60\n",
      "31.82\n",
      "32.06\n",
      "29.62\n",
      "Midjourney V5\n",
      "32.33\n",
      "34.05\n",
      "32.47\n",
      "32.24\n",
      "30.56\n",
      "CogView4-6B\n",
      "32.31\n",
      "33.23\n",
      "32.60\n",
      "32.89\n",
      "30.52\n",
      "Flux.1-dev\n",
      "32.47\n",
      "33.87\n",
      "32.27\n",
      "32.62\n",
      "31.11\n",
      "stable cascade\n",
      "32.95\n",
      "34.58\n",
      "33.13\n",
      "33.29\n",
      "30.78\n",
      "HiDream-I1\n",
      "33.82\n",
      "35.05\n",
      "33.74\n",
      "33.88\n",
      "32.61\n",
      "License Agreement\n",
      "The Transformer models in this repository are licensed under the MIT License. The VAE is from\n",
      "FLUX.1 [schnell]\n",
      ", and the text encoders from\n",
      "google/t5-v1_1-xxl\n",
      "and\n",
      "meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      ". Please follow the license terms specified for these components. You own all content you create with this model. You can use your generated content freely, but you must comply with this license agreement. You are responsible for how you use the models. Do not create illegal content, harmful material, personal information that could harm others, false information, or content targeting vulnerable groups.\n",
      "Acknowledgements\n",
      "The VAE component is from\n",
      "FLUX.1 [schnell]\n",
      ", licensed under Apache 2.0.\n",
      "The text encoders are from\n",
      "google/t5-v1_1-xxl\n",
      "(licensed under Apache 2.0) and\n",
      "meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "(licensed under the Llama 3.1 Community License Agreement).\n",
      "Downloads last month\n",
      "31,199\n",
      "Inference Providers\n",
      "NEW\n",
      "fal\n",
      "Text-to-Image\n",
      "Examples\n",
      "Compute\n",
      "View Code\n",
      "Maximize\n",
      "Model tree for\n",
      "HiDream-ai/HiDream-I1-Full\n",
      "Adapters\n",
      "15 models\n",
      "Finetunes\n",
      "1 model\n",
      "Quantizations\n",
      "4 models\n",
      "Spaces using\n",
      "HiDream-ai/HiDream-I1-Full\n",
      "64\n",
      "‚ô®Ô∏è‚ô®Ô∏è‚ô®Ô∏è\n",
      "Nymbo/Serverless-ImgGen-Hub\n",
      "üñº\n",
      "wavespeed/hidream-arena\n",
      "üòª\n",
      "adventus/DazDinGoFLX3\n",
      "ü¶Ä\n",
      "Blessed304/HiDream-ai-HiDream-I1-Full\n",
      "üìä\n",
      "sakthivelt/HiDream-ai-HiDream-I1-Full\n",
      "üê®\n",
      "flowersniffin/HiDream-ai-HiDream-I1-Full\n",
      "üöÄ\n",
      "kosmicoctopus/HiDream-ai-HiDream-I1-Full\n",
      "üìö\n",
      "MavrickMixx/HiDream-ai-HiDream-I1-Full\n",
      "üèÉ\n",
      "BlackGoku7/HiDream-ai-HiDream-I1-Full\n",
      "‚ö°\n",
      "pangolins/HiDream-ai-HiDream-I1-Full\n",
      "üöÄ\n",
      "Kino09/HiDream-ai-HiDream-I1-Full\n",
      "üöÄ\n",
      "Kino09/CKV-ai\n",
      "+ 59 Spaces\n",
      "+ 52 Spaces\n",
      "Collection including\n",
      "HiDream-ai/HiDream-I1-Full\n",
      "HiDream-I1\n",
      "Collection\n",
      "A collections of HiDream-I1 models.\n",
      "‚Ä¢\n",
      "4 items\n",
      "‚Ä¢\n",
      "Updated\n",
      "18 days ago\n",
      "‚Ä¢\n",
      "26\n",
      "System theme\n",
      "Company\n",
      "TOS\n",
      "Privacy\n",
      "About\n",
      "Jobs\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Pricing\n",
      "Docs\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "Dia 1.6B - a Hugging Face Space by nari-labs\n",
      "Webpage Contents:\n",
      "Spaces\n",
      "nari-labs\n",
      "/\n",
      "Dia-1.6B\n",
      "like\n",
      "700\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "App\n",
      "Files\n",
      "Files\n",
      "Community\n",
      "8\n",
      "Fetching metadata from the HF Docker repository...\n",
      "Refreshing\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "DeepSite - a Hugging Face Space by enzostvs\n",
      "Webpage Contents:\n",
      "Spaces\n",
      "enzostvs\n",
      "/\n",
      "deepsite\n",
      "like\n",
      "5.32k\n",
      "Running\n",
      "App\n",
      "Files\n",
      "Files\n",
      "Community\n",
      "94\n",
      "Fetching metadata from the HF Docker repository...\n",
      "Refreshing\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "Kolors Virtual Try-On - a Hugging Face Space by Kwai-Kolors\n",
      "Webpage Contents:\n",
      "Spaces\n",
      "Kwai-Kolors\n",
      "/\n",
      "Kolors-Virtual-Try-On\n",
      "like\n",
      "8.53k\n",
      "Running\n",
      "on\n",
      "CPU Upgrade\n",
      "App\n",
      "Files\n",
      "Files\n",
      "Community\n",
      "356\n",
      "Fetching metadata from the HF Docker repository...\n",
      "Refreshing\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "Spaces - Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Spaces\n",
      "¬∑\n",
      "The AI App Directory\n",
      "New Space\n",
      "What is Spaces?\n",
      "Image Generation\n",
      "Video Generation\n",
      "Text Generation\n",
      "Language Translation\n",
      "Speech Synthesis\n",
      "3D Modeling\n",
      "Object Detection\n",
      "Text Analysis\n",
      "Image Editing\n",
      "Code Generation\n",
      "Question Answering\n",
      "Data Visualization\n",
      "Voice Cloning\n",
      "Background Removal\n",
      "Image Upscaling\n",
      "OCR\n",
      "Document Analysis\n",
      "Visual QA\n",
      "Image Captioning\n",
      "Chatbots\n",
      "Sentiment Analysis\n",
      "Text Summarization\n",
      "Music Generation\n",
      "Medical Imaging\n",
      "Financial Analysis\n",
      "Game AI\n",
      "Model Benchmarking\n",
      "Fine Tuning Tools\n",
      "Dataset Creation\n",
      "Pose Estimation\n",
      "Face Recognition\n",
      "Anomaly Detection\n",
      "Recommendation Systems\n",
      "Character Animation\n",
      "Style Transfer\n",
      "Image\n",
      "Spaces of the week\n",
      "21 Apr 2025\n",
      "Sort:¬†\n",
      "\t\tRelevance\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "700\n",
      "Dia 1.6B\n",
      "üëØ\n",
      "Generate realistic dialogue from a script, using Dia!\n",
      "nari-labs\n",
      "about 9 hours ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "48\n",
      "Chat with Bitnet-b1.58-2B-4T\n",
      "üëæ\n",
      "Chat with Microsoft's 1.58bit Bitnet model!\n",
      "suayptalha\n",
      "6 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "42\n",
      "Vevo for Zero-shot VC, TTS, and More\n",
      "üê†\n",
      "Controllable Zero-Shot Voice Imitation\n",
      "amphion\n",
      "3 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "20\n",
      "MESA\n",
      "üèî\n",
      "Text-to-terrain model (reflectance and elevation)\n",
      "mikonvergence\n",
      "10 days ago\n",
      "Running\n",
      "66\n",
      "MotionShop2\n",
      "üèÉ\n",
      "Replace characters in a video with characters in photos\n",
      "3DAIGC\n",
      "11 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "49\n",
      "DetailGen3D\n",
      "üëÅ\n",
      "Generate detailed 3D models from reference images and coarse models\n",
      "VAST-AI\n",
      "8 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "54\n",
      "FLUX.1 Dev ControlNet Union Pro 2.0\n",
      "üî•\n",
      "Create images using prompts and control images\n",
      "Shakker-Labs\n",
      "9 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "48\n",
      "AudioX\n",
      "üëÄ\n",
      "Generate audio and video from text prompts\n",
      "Zeyue7\n",
      "10 days ago\n",
      "All running apps, trending first\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "700\n",
      "Dia 1.6B\n",
      "üëØ\n",
      "Generate realistic dialogue from a script, using Dia!\n",
      "nari-labs\n",
      "about 9 hours ago\n",
      "Running\n",
      "5.32k\n",
      "DeepSite\n",
      "üê≥\n",
      "Generate any application with DeepSeek\n",
      "enzostvs\n",
      "9 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "255\n",
      "InstantCharacter\n",
      "üê¢\n",
      "Customize characters with prompts and styles\n",
      "InstantX\n",
      "6 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "586\n",
      "UNO FLUX\n",
      "‚ö°\n",
      "Generate customized images using text and multiple images\n",
      "bytedance-research\n",
      "14 days ago\n",
      "Running\n",
      "on\n",
      "CPU Upgrade\n",
      "8.53k\n",
      "Kolors Virtual Try-On\n",
      "üëï\n",
      "Try on clothes on images of people\n",
      "Kwai-Kolors\n",
      "Sep 18, 2024\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "1.34k\n",
      "EasyControl Ghibli\n",
      "ü¶Ä\n",
      "New Ghibli EasyControl model is now released!!\n",
      "jamesliu1217\n",
      "15 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "95\n",
      "Describe Anything\n",
      "‚ö°\n",
      "Describe parts of images using text prompts\n",
      "nvidia\n",
      "2 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "95\n",
      "FramePackÂõæÂÉèÂà∞ËßÜÈ¢ëÁîüÊàê(5ÁßíÈôêÂà∂Áâà)\n",
      "üé¨\n",
      "Generate video from an image\n",
      "lisonallen\n",
      "7 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "1.53k\n",
      "FLUX LoRa the Explorer\n",
      "üèÜ\n",
      "Generate images based on prompts and LoRA models\n",
      "multimodalart\n",
      "Feb 17\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "300\n",
      "OminiControl Art\n",
      "üé®\n",
      "Transform images into artistic styles like Studio Ghibli\n",
      "Yuanshi\n",
      "5 days ago\n",
      "Running\n",
      "on\n",
      "CPU Upgrade\n",
      "297\n",
      "HiDream I1 Dev\n",
      "üöÄ\n",
      "Generate high-quality images from text descriptions\n",
      "HiDream-ai\n",
      "13 days ago\n",
      "Running\n",
      "on\n",
      "CPU Upgrade\n",
      "5.5k\n",
      "MTEB Leaderboard\n",
      "ü•á\n",
      "Embedding Leaderboard\n",
      "mteb\n",
      "9 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "3.41k\n",
      "IC Light V2\n",
      "üìà\n",
      "Execute custom Python scripts from environment variables\n",
      "lllyasviel\n",
      "Oct 26, 2024\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "344\n",
      "SanaSprint\n",
      "üëÅ\n",
      "Ultra fast high quality image generation\n",
      "Efficient-Large-Model\n",
      "20 days ago\n",
      "Running\n",
      "66\n",
      "MotionShop2\n",
      "üèÉ\n",
      "Replace characters in a video with characters in photos\n",
      "3DAIGC\n",
      "11 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "4.69k\n",
      "FLUX.1 [Schnell]\n",
      "üèé\n",
      "Generate images from text prompts\n",
      "black-forest-labs\n",
      "Aug 9, 2024\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "8.15k\n",
      "FLUX.1 [dev]\n",
      "üñ•\n",
      "Generate images from text prompts\n",
      "black-forest-labs\n",
      "11 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "682\n",
      "TripoSG\n",
      "üîÆ\n",
      "Generate a 3D model from an image\n",
      "VAST-AI\n",
      "18 days ago\n",
      "Running\n",
      "345\n",
      "DeepSite Gallery\n",
      "üêã\n",
      "Browse apps made with DeepSite\n",
      "victor\n",
      "18 days ago\n",
      "Running\n",
      "on\n",
      "CPU Upgrade\n",
      "9.97k\n",
      "AI Comic Factory\n",
      "üë©\n",
      "Create your own AI comic with a single prompt\n",
      "jbilcke-hf\n",
      "Oct 15, 2024\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "1.66k\n",
      "Background Removal\n",
      "üåò\n",
      "Remove backgrounds from images\n",
      "not-lain\n",
      "Jan 2\n",
      "Running\n",
      "1.16k\n",
      "InstantCoder\n",
      "ü¶Ä\n",
      "Generate app code from ideas\n",
      "osanseviero\n",
      "Mar 25\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "48\n",
      "Chat with Bitnet-b1.58-2B-4T\n",
      "üëæ\n",
      "Chat with Microsoft's 1.58bit Bitnet model!\n",
      "suayptalha\n",
      "6 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "49\n",
      "DetailGen3D\n",
      "üëÅ\n",
      "Generate detailed 3D models from reference images and coarse models\n",
      "VAST-AI\n",
      "8 days ago\n",
      "Previous\n",
      "1\n",
      "2\n",
      "3\n",
      "...\n",
      "100\n",
      "Next\n",
      "System theme\n",
      "Company\n",
      "TOS\n",
      "Privacy\n",
      "About\n",
      "Jobs\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Pricing\n",
      "Docs\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "404 ‚Äì Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "404\n",
      "Sorry, we can't find the page you are looking for.\n",
      "System theme\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Tasks\n",
      "Inference Endpoints\n",
      "HuggingChat\n",
      "Company\n",
      "About\n",
      "Brand assets\n",
      "Terms of service\n",
      "Privacy\n",
      "Jobs\n",
      "Press\n",
      "Resources\n",
      "Learn\n",
      "Documentation\n",
      "Blog\n",
      "Forum\n",
      "Service Status\n",
      "Social\n",
      "GitHub\n",
      "Twitter\n",
      "LinkedIn\n",
      "Discord\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "404 ‚Äì Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "404\n",
      "Sorry, we can't find the page you are looking for.\n",
      "System theme\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Tasks\n",
      "Inference Endpoints\n",
      "HuggingChat\n",
      "Company\n",
      "About\n",
      "Brand assets\n",
      "Terms of service\n",
      "Privacy\n",
      "Jobs\n",
      "Press\n",
      "Resources\n",
      "Learn\n",
      "Documentation\n",
      "Blog\n",
      "Forum\n",
      "Service Status\n",
      "Social\n",
      "GitHub\n",
      "Twitter\n",
      "LinkedIn\n",
      "Discord\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "404 ‚Äì Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "404\n",
      "Sorry, we can't find the page you are looking for.\n",
      "System theme\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Tasks\n",
      "Inference Endpoints\n",
      "HuggingChat\n",
      "Company\n",
      "About\n",
      "Brand assets\n",
      "Terms of service\n",
      "Privacy\n",
      "Jobs\n",
      "Press\n",
      "Resources\n",
      "Learn\n",
      "Documentation\n",
      "Blog\n",
      "Forum\n",
      "Service Status\n",
      "Social\n",
      "GitHub\n",
      "Twitter\n",
      "LinkedIn\n",
      "Discord\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "404 ‚Äì Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "404\n",
      "Sorry, we can't find the page you are looking for.\n",
      "System theme\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Tasks\n",
      "Inference Endpoints\n",
      "HuggingChat\n",
      "Company\n",
      "About\n",
      "Brand assets\n",
      "Terms of service\n",
      "Privacy\n",
      "Jobs\n",
      "Press\n",
      "Resources\n",
      "Learn\n",
      "Documentation\n",
      "Blog\n",
      "Forum\n",
      "Service Status\n",
      "Social\n",
      "GitHub\n",
      "Twitter\n",
      "LinkedIn\n",
      "Discord\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "404 ‚Äì Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "404\n",
      "Sorry, we can't find the page you are looking for.\n",
      "System theme\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Tasks\n",
      "Inference Endpoints\n",
      "HuggingChat\n",
      "Company\n",
      "About\n",
      "Brand assets\n",
      "Terms of service\n",
      "Privacy\n",
      "Jobs\n",
      "Press\n",
      "Resources\n",
      "Learn\n",
      "Documentation\n",
      "Blog\n",
      "Forum\n",
      "Service Status\n",
      "Social\n",
      "GitHub\n",
      "Twitter\n",
      "LinkedIn\n",
      "Discord\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "404 ‚Äì Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "404\n",
      "Sorry, we can't find the page you are looking for.\n",
      "System theme\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Tasks\n",
      "Inference Endpoints\n",
      "HuggingChat\n",
      "Company\n",
      "About\n",
      "Brand assets\n",
      "Terms of service\n",
      "Privacy\n",
      "Jobs\n",
      "Press\n",
      "Resources\n",
      "Learn\n",
      "Documentation\n",
      "Blog\n",
      "Forum\n",
      "Service Status\n",
      "Social\n",
      "GitHub\n",
      "Twitter\n",
      "LinkedIn\n",
      "Discord\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "Hugging Face ‚Äì Pricing\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Pricing\n",
      "Leveling up AI collaboration and compute.\n",
      "Users and organizations already use the Hub as a collaboration platform,\n",
      "we‚Äôre making it easy to seamlessly and scalably launch ML compute directly from the Hub.\n",
      "HF Hub\n",
      "Collaborate on Machine Learning\n",
      "Host unlimited public models, datasets\n",
      "Create unlimited orgs with no member limits\n",
      "Access the latest ML tools and open source\n",
      "Community support\n",
      "Forever\n",
      "Free\n",
      "PRO\n",
      "Pro Account\n",
      "Unlock advanced HF features\n",
      "ZeroGPU and Dev Mode for Spaces\n",
      "Free credits across all Inference Providers\n",
      "Get early access to upcoming features\n",
      "Show your support with a Pro badge\n",
      "Subscribe for\n",
      "$9\n",
      "/month\n",
      "Enterprise Hub\n",
      "Accelerate your AI roadmap\n",
      "SSO and SAML support\n",
      "Select data location with Storage Regions\n",
      "Precise actions reviews with Audit logs\n",
      "Granular access control with Resource groups\n",
      "Centralized token control and approval\n",
      "Dataset Viewer for private datasets\n",
      "Advanced compute options for Spaces\n",
      "5x more ZeroGPU quota for all org members\n",
      "Deploy Inference on your own Infra\n",
      "Managed billing with yearly commits\n",
      "Priority support\n",
      "Starting at\n",
      "$20\n",
      "per user per month\n",
      "Spaces Hardware\n",
      "Upgrade your Space compute\n",
      "Free CPUs\n",
      "Build more advanced Spaces\n",
      "7 optimized hardware available\n",
      "From CPU to GPU to Accelerators\n",
      "Starting at\n",
      "$0\n",
      "/hour\n",
      "Inference Endpoints\n",
      "Deploy models on fully managed infrastructure\n",
      "Deploy dedicated Endpoints in seconds\n",
      "Keep your costs low\n",
      "Fully-managed autoscaling\n",
      "Enterprise security\n",
      "Starting at\n",
      "$0.032\n",
      "/hour\n",
      "Need support to accelerate AI in your organization? View our\n",
      "Expert Support\n",
      ".\n",
      "Hugging Face Hub\n",
      "free\n",
      "The HF Hub is the central place to explore, experiment, collaborate and build technology with Machine\n",
      "\t\t\t\t\tLearning.\n",
      "Join the open source Machine Learning movement!\n",
      "‚Üí\n",
      "Sign Up\n",
      "Create with ML\n",
      "Packed with ML features, like model eval, dataset viewer and much more.\n",
      "Collaborate\n",
      "Git based and designed for collaboration at its core.\n",
      "Play and learn\n",
      "Learn by experimenting and sharing with our awesome community.\n",
      "Build your ML portfolio\n",
      "Share your work with the world and build your own ML profile.\n",
      "Spaces Hardware\n",
      "Starting at $0\n",
      "Spaces are one of the most popular ways to share ML applications and demos with the world.\n",
      "Upgrade your Spaces with our selection of custom on-demand hardware:\n",
      "‚Üí\n",
      "Get started with Spaces\n",
      "Name\n",
      "CPU\n",
      "Memory\n",
      "Accelerator\n",
      "VRAM\n",
      "Hourly price\n",
      "CPU Basic\n",
      "2 vCPU\n",
      "16 GB\n",
      "-\n",
      "-\n",
      "FREE\n",
      "CPU Upgrade\n",
      "8 vCPU\n",
      "32 GB\n",
      "-\n",
      "-\n",
      "$0.03\n",
      "Nvidia T4 - small\n",
      "4 vCPU\n",
      "15 GB\n",
      "Nvidia T4\n",
      "16 GB\n",
      "$0.40\n",
      "Nvidia T4 - medium\n",
      "8 vCPU\n",
      "30 GB\n",
      "Nvidia T4\n",
      "16 GB\n",
      "$0.60\n",
      "1x Nvidia L4\n",
      "8 vCPU\n",
      "30 GB\n",
      "Nvidia L4\n",
      "24 GB\n",
      "$0.80\n",
      "4x Nvidia L4\n",
      "48 vCPU\n",
      "186 GB\n",
      "Nvidia L4\n",
      "96 GB\n",
      "$3.80\n",
      "1x Nvidia L40S\n",
      "8 vCPU\n",
      "62 GB\n",
      "Nvidia L4\n",
      "48 GB\n",
      "$1.80\n",
      "4x Nvidia L40S\n",
      "48 vCPU\n",
      "382 GB\n",
      "Nvidia L4\n",
      "192 GB\n",
      "$8.30\n",
      "8x Nvidia L40S\n",
      "192 vCPU\n",
      "1534 GB\n",
      "Nvidia L4\n",
      "384 GB\n",
      "$23.50\n",
      "Nvidia A10G - small\n",
      "4 vCPU\n",
      "15 GB\n",
      "Nvidia A10G\n",
      "24 GB\n",
      "$1.00\n",
      "Nvidia A10G - large\n",
      "12 vCPU\n",
      "46 GB\n",
      "Nvidia A10G\n",
      "24 GB\n",
      "$1.50\n",
      "2x Nvidia A10G - large\n",
      "24 vCPU\n",
      "92 GB\n",
      "Nvidia A10G\n",
      "48 GB\n",
      "$3.00\n",
      "4x Nvidia A10G - large\n",
      "48 vCPU\n",
      "184 GB\n",
      "Nvidia A10G\n",
      "96 GB\n",
      "$5.00\n",
      "Nvidia A100 - large\n",
      "12 vCPU\n",
      "142 GB\n",
      "Nvidia A100\n",
      "80 GB\n",
      "$4.00\n",
      "Custom\n",
      "on demand\n",
      "on demand\n",
      "on demand\n",
      "on demand\n",
      "on demand\n",
      "Spaces Persistent Storage\n",
      "All Spaces get ephemeral storage for free but you can upgrade and add persistent storage at any time.\n",
      "Name\n",
      "Storage\n",
      "Monthly price\n",
      "Small\n",
      "20 GB\n",
      "$5\n",
      "Medium\n",
      "150 GB\n",
      "$25\n",
      "Large\n",
      "1 TB\n",
      "$100\n",
      "Building something cool as a side project? We also offer community GPU grants.\n",
      "Inference Endpoints\n",
      "Starting at $0.033/hour\n",
      "Inference Endpoints (dedicated) offers a secure production solution to easily deploy any ML model on dedicated\n",
      "\t\t\t\t\tand autoscaling infrastructure, right from the HF Hub.\n",
      "‚Üí\n",
      "Learn more\n",
      "CPU\n",
      "instances\n",
      "Provider\n",
      "Architecture\n",
      "vCPUs\n",
      "Memory\n",
      "Hourly rate\n",
      "aws\n",
      "Intel Sapphire Rapids\n",
      "1\n",
      "2GB\n",
      "$0.03\n",
      "2\n",
      "4GB\n",
      "$0.07\n",
      "4\n",
      "8GB\n",
      "$0.13\n",
      "8\n",
      "16GB\n",
      "$0.27\n",
      "16\n",
      "32GB\n",
      "$0.54\n",
      "azure\n",
      "Intel Xeon\n",
      "1\n",
      "2GB\n",
      "$0.06\n",
      "2\n",
      "4GB\n",
      "$0.12\n",
      "4\n",
      "8GB\n",
      "$0.24\n",
      "8\n",
      "16GB\n",
      "$0.48\n",
      "gcp\n",
      "Intel Sapphire Rapids\n",
      "1\n",
      "2GB\n",
      "$0.05\n",
      "2\n",
      "4GB\n",
      "$0.10\n",
      "4\n",
      "8GB\n",
      "$0.20\n",
      "8\n",
      "16GB\n",
      "$0.40\n",
      "Accelerator\n",
      "instances\n",
      "Provider\n",
      "Architecture\n",
      "Topology\n",
      "Accelerator Memory\n",
      "Hourly rate\n",
      "aws\n",
      "Inf2\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNeuron\n",
      "x1\n",
      "14.5GB\n",
      "$0.75\n",
      "x12\n",
      "760GB\n",
      "$12.00\n",
      "gcp\n",
      "TPU\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tv5e\n",
      "1x1\n",
      "16GB\n",
      "$1.20\n",
      "2x2\n",
      "64GB\n",
      "$4.75\n",
      "2x4\n",
      "128GB\n",
      "$9.50\n",
      "GPU\n",
      "instances\n",
      "Provider\n",
      "Architecture\n",
      "GPUs\n",
      "GPU Memory\n",
      "Hourly rate\n",
      "aws\n",
      "NVIDIA T4\n",
      "1\n",
      "14GB\n",
      "$0.50\n",
      "4\n",
      "56GB\n",
      "$3.00\n",
      "aws\n",
      "NVIDIA L4\n",
      "1\n",
      "24GB\n",
      "$0.80\n",
      "4\n",
      "96GB\n",
      "$3.80\n",
      "aws\n",
      "NVIDIA L40S\n",
      "1\n",
      "48GB\n",
      "$1.80\n",
      "4\n",
      "192GB\n",
      "$8.30\n",
      "8\n",
      "384GB\n",
      "$23.50\n",
      "aws\n",
      "NVIDIA A10G\n",
      "1\n",
      "24GB\n",
      "$1.00\n",
      "4\n",
      "96GB\n",
      "$5.00\n",
      "aws\n",
      "NVIDIA A100\n",
      "1\n",
      "80GB\n",
      "$4.00\n",
      "2\n",
      "160GB\n",
      "$8.00\n",
      "4\n",
      "320GB\n",
      "$16.00\n",
      "8\n",
      "640GB\n",
      "$32.00\n",
      "aws\n",
      "NVIDIA H200\n",
      "1\n",
      "141GB\n",
      "$5.00\n",
      "2\n",
      "282GB\n",
      "$10.00\n",
      "4\n",
      "564GB\n",
      "$20.00\n",
      "8\n",
      "1128GB\n",
      "$40.00\n",
      "gcp\n",
      "NVIDIA T4\n",
      "1\n",
      "16GB\n",
      "$0.50\n",
      "gcp\n",
      "NVIDIA L4\n",
      "1\n",
      "24GB\n",
      "$0.70\n",
      "4\n",
      "96GB\n",
      "$3.80\n",
      "gcp\n",
      "NVIDIA A100\n",
      "1\n",
      "80GB\n",
      "$3.60\n",
      "2\n",
      "160GB\n",
      "$7.20\n",
      "4\n",
      "320GB\n",
      "$14.40\n",
      "8\n",
      "640GB\n",
      "$28.80\n",
      "gcp\n",
      "NVIDIA H100\n",
      "1\n",
      "80GB\n",
      "$10.00\n",
      "2\n",
      "160GB\n",
      "$20.00\n",
      "4\n",
      "320GB\n",
      "$40.00\n",
      "8\n",
      "640GB\n",
      "$80.00\n",
      "Pro Account\n",
      "PRO\n",
      "A monthly subscription to access powerful features.\n",
      "‚Üí\n",
      "Get Pro\n",
      "($9/month)\n",
      "ZeroGPU\n",
      ": Get 5x usage quota and highest GPU queue priority\n",
      "Spaces Hosting\n",
      ": Create ZeroGPU Spaces with A100 hardware\n",
      "Spaces Dev Mode\n",
      ": Fast iterations via SSH/VS Code for Spaces\n",
      "Inference Providers\n",
      ": Get $2 included credits across all Inference Providers\n",
      "Dataset Viewer\n",
      ": Activate it on private datasets\n",
      "Blog Articles\n",
      ": Publish articles to the Hugging Face blog\n",
      "Social Posts\n",
      ": Share short updates with the community\n",
      "Features Preview\n",
      ": Get early access to upcoming\n",
      "\t\t\t\t\t\t\t\t\t\tfeatures\n",
      "PRO\n",
      "Badge\n",
      ":\n",
      "\t\t\t\t\t\t\t\t\t\tShow your support on your profile\n",
      "System theme\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Tasks\n",
      "Inference Endpoints\n",
      "HuggingChat\n",
      "Company\n",
      "About\n",
      "Brand assets\n",
      "Terms of service\n",
      "Privacy\n",
      "Jobs\n",
      "Press\n",
      "Resources\n",
      "Learn\n",
      "Documentation\n",
      "Blog\n",
      "Forum\n",
      "Service Status\n",
      "Social\n",
      "GitHub\n",
      "Twitter\n",
      "LinkedIn\n",
      "Discord\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "Hugging Face ‚Äì Pricing\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Pricing\n",
      "Leveling up AI collaboration and compute.\n",
      "Users and organizations already use the Hub as a collaboration platform,\n",
      "we‚Äôre making it easy to seamlessly and scalably launch ML compute directly from the Hub.\n",
      "HF Hub\n",
      "Collaborate on Machine Learning\n",
      "Host unlimited public models, datasets\n",
      "Create unlimited orgs with no member limits\n",
      "Access the latest ML tools and open source\n",
      "Community support\n",
      "Forever\n",
      "Free\n",
      "PRO\n",
      "Pro Account\n",
      "Unlock advanced HF features\n",
      "ZeroGPU and Dev Mode for Spaces\n",
      "Free credits across all Inference Providers\n",
      "Get early access to upcoming features\n",
      "Show your support with a Pro badge\n",
      "Subscribe for\n",
      "$9\n",
      "/month\n",
      "Enterprise Hub\n",
      "Accelerate your AI roadmap\n",
      "SSO and SAML support\n",
      "Select data location with Storage Regions\n",
      "Precise actions reviews with Audit logs\n",
      "Granular access control with Resource groups\n",
      "Centralized token control and approval\n",
      "Dataset Viewer for private datasets\n",
      "Advanced compute options for Spaces\n",
      "5x more ZeroGPU quota for all org members\n",
      "Deploy Inference on your own Infra\n",
      "Managed billing with yearly commits\n",
      "Priority support\n",
      "Starting at\n",
      "$20\n",
      "per user per month\n",
      "Spaces Hardware\n",
      "Upgrade your Space compute\n",
      "Free CPUs\n",
      "Build more advanced Spaces\n",
      "7 optimized hardware available\n",
      "From CPU to GPU to Accelerators\n",
      "Starting at\n",
      "$0\n",
      "/hour\n",
      "Inference Endpoints\n",
      "Deploy models on fully managed infrastructure\n",
      "Deploy dedicated Endpoints in seconds\n",
      "Keep your costs low\n",
      "Fully-managed autoscaling\n",
      "Enterprise security\n",
      "Starting at\n",
      "$0.032\n",
      "/hour\n",
      "Need support to accelerate AI in your organization? View our\n",
      "Expert Support\n",
      ".\n",
      "Hugging Face Hub\n",
      "free\n",
      "The HF Hub is the central place to explore, experiment, collaborate and build technology with Machine\n",
      "\t\t\t\t\tLearning.\n",
      "Join the open source Machine Learning movement!\n",
      "‚Üí\n",
      "Sign Up\n",
      "Create with ML\n",
      "Packed with ML features, like model eval, dataset viewer and much more.\n",
      "Collaborate\n",
      "Git based and designed for collaboration at its core.\n",
      "Play and learn\n",
      "Learn by experimenting and sharing with our awesome community.\n",
      "Build your ML portfolio\n",
      "Share your work with the world and build your own ML profile.\n",
      "Spaces Hardware\n",
      "Starting at $0\n",
      "Spaces are one of the most popular ways to share ML applications and demos with the world.\n",
      "Upgrade your Spaces with our selection of custom on-demand hardware:\n",
      "‚Üí\n",
      "Get started with Spaces\n",
      "Name\n",
      "CPU\n",
      "Memory\n",
      "Accelerator\n",
      "VRAM\n",
      "Hourly price\n",
      "CPU Basic\n",
      "2 vCPU\n",
      "16 GB\n",
      "-\n",
      "-\n",
      "FREE\n",
      "CPU Upgrade\n",
      "8 vCPU\n",
      "32 GB\n",
      "-\n",
      "-\n",
      "$0.03\n",
      "Nvidia T4 - small\n",
      "4 vCPU\n",
      "15 GB\n",
      "Nvidia T4\n",
      "16 GB\n",
      "$0.40\n",
      "Nvidia T4 - medium\n",
      "8 vCPU\n",
      "30 GB\n",
      "Nvidia T4\n",
      "16 GB\n",
      "$0.60\n",
      "1x Nvidia L4\n",
      "8 vCPU\n",
      "30 GB\n",
      "Nvidia L4\n",
      "24 GB\n",
      "$0.80\n",
      "4x Nvidia L4\n",
      "48 vCPU\n",
      "186 GB\n",
      "Nvidia L4\n",
      "96 GB\n",
      "$3.80\n",
      "1x Nvidia L40S\n",
      "8 vCPU\n",
      "62 GB\n",
      "Nvidia L4\n",
      "48 GB\n",
      "$1.80\n",
      "4x Nvidia L40S\n",
      "48 vCPU\n",
      "382 GB\n",
      "Nvidia L4\n",
      "192 GB\n",
      "$8.30\n",
      "8x Nvidia L40S\n",
      "192 vCPU\n",
      "1534 GB\n",
      "Nvidia L4\n",
      "384 GB\n",
      "$23.50\n",
      "Nvidia A10G - small\n",
      "4 vCPU\n",
      "15 GB\n",
      "Nvidia A10G\n",
      "24 GB\n",
      "$1.00\n",
      "Nvidia A10G - large\n",
      "12 vCPU\n",
      "46 GB\n",
      "Nvidia A10G\n",
      "24 GB\n",
      "$1.50\n",
      "2x Nvidia A10G - large\n",
      "24 vCPU\n",
      "92 GB\n",
      "Nvidia A10G\n",
      "48 GB\n",
      "$3.00\n",
      "4x Nvidia A10G - large\n",
      "48 vCPU\n",
      "184 GB\n",
      "Nvidia A10G\n",
      "96 GB\n",
      "$5.00\n",
      "Nvidia A100 - large\n",
      "12 vCPU\n",
      "142 GB\n",
      "Nvidia A100\n",
      "80 GB\n",
      "$4.00\n",
      "Custom\n",
      "on demand\n",
      "on demand\n",
      "on demand\n",
      "on demand\n",
      "on demand\n",
      "Spaces Persistent Storage\n",
      "All Spaces get ephemeral storage for free but you can upgrade and add persistent storage at any time.\n",
      "Name\n",
      "Storage\n",
      "Monthly price\n",
      "Small\n",
      "20 GB\n",
      "$5\n",
      "Medium\n",
      "150 GB\n",
      "$25\n",
      "Large\n",
      "1 TB\n",
      "$100\n",
      "Building something cool as a side project? We also offer community GPU grants.\n",
      "Inference Endpoints\n",
      "Starting at $0.033/hour\n",
      "Inference Endpoints (dedicated) offers a secure production solution to easily deploy any ML model on dedicated\n",
      "\t\t\t\t\tand autoscaling infrastructure, right from the HF Hub.\n",
      "‚Üí\n",
      "Learn more\n",
      "CPU\n",
      "instances\n",
      "Provider\n",
      "Architecture\n",
      "vCPUs\n",
      "Memory\n",
      "Hourly rate\n",
      "aws\n",
      "Intel Sapphire Rapids\n",
      "1\n",
      "2GB\n",
      "$0.03\n",
      "2\n",
      "4GB\n",
      "$0.07\n",
      "4\n",
      "8GB\n",
      "$0.13\n",
      "8\n",
      "16GB\n",
      "$0.27\n",
      "16\n",
      "32GB\n",
      "$0.54\n",
      "azure\n",
      "Intel Xeon\n",
      "1\n",
      "2GB\n",
      "$0.06\n",
      "2\n",
      "4GB\n",
      "$0.12\n",
      "4\n",
      "8GB\n",
      "$0.24\n",
      "8\n",
      "16GB\n",
      "$0.48\n",
      "gcp\n",
      "Intel Sapphire Rapids\n",
      "1\n",
      "2GB\n",
      "$0.05\n",
      "2\n",
      "4GB\n",
      "$0.10\n",
      "4\n",
      "8GB\n",
      "$0.20\n",
      "8\n",
      "16GB\n",
      "$0.40\n",
      "Accelerator\n",
      "instances\n",
      "Provider\n",
      "Architecture\n",
      "Topology\n",
      "Accelerator Memory\n",
      "Hourly rate\n",
      "aws\n",
      "Inf2\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNeuron\n",
      "x1\n",
      "14.5GB\n",
      "$0.75\n",
      "x12\n",
      "760GB\n",
      "$12.00\n",
      "gcp\n",
      "TPU\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tv5e\n",
      "1x1\n",
      "16GB\n",
      "$1.20\n",
      "2x2\n",
      "64GB\n",
      "$4.75\n",
      "2x4\n",
      "128GB\n",
      "$9.50\n",
      "GPU\n",
      "instances\n",
      "Provider\n",
      "Architecture\n",
      "GPUs\n",
      "GPU Memory\n",
      "Hourly rate\n",
      "aws\n",
      "NVIDIA T4\n",
      "1\n",
      "14GB\n",
      "$0.50\n",
      "4\n",
      "56GB\n",
      "$3.00\n",
      "aws\n",
      "NVIDIA L4\n",
      "1\n",
      "24GB\n",
      "$0.80\n",
      "4\n",
      "96GB\n",
      "$3.80\n",
      "aws\n",
      "NVIDIA L40S\n",
      "1\n",
      "48GB\n",
      "$1.80\n",
      "4\n",
      "192GB\n",
      "$8.30\n",
      "8\n",
      "384GB\n",
      "$23.50\n",
      "aws\n",
      "NVIDIA A10G\n",
      "1\n",
      "24GB\n",
      "$1.00\n",
      "4\n",
      "96GB\n",
      "$5.00\n",
      "aws\n",
      "NVIDIA A100\n",
      "1\n",
      "80GB\n",
      "$4.00\n",
      "2\n",
      "160GB\n",
      "$8.00\n",
      "4\n",
      "320GB\n",
      "$16.00\n",
      "8\n",
      "640GB\n",
      "$32.00\n",
      "aws\n",
      "NVIDIA H200\n",
      "1\n",
      "141GB\n",
      "$5.00\n",
      "2\n",
      "282GB\n",
      "$10.00\n",
      "4\n",
      "564GB\n",
      "$20.00\n",
      "8\n",
      "1128GB\n",
      "$40.00\n",
      "gcp\n",
      "NVIDIA T4\n",
      "1\n",
      "16GB\n",
      "$0.50\n",
      "gcp\n",
      "NVIDIA L4\n",
      "1\n",
      "24GB\n",
      "$0.70\n",
      "4\n",
      "96GB\n",
      "$3.80\n",
      "gcp\n",
      "NVIDIA A100\n",
      "1\n",
      "80GB\n",
      "$3.60\n",
      "2\n",
      "160GB\n",
      "$7.20\n",
      "4\n",
      "320GB\n",
      "$14.40\n",
      "8\n",
      "640GB\n",
      "$28.80\n",
      "gcp\n",
      "NVIDIA H100\n",
      "1\n",
      "80GB\n",
      "$10.00\n",
      "2\n",
      "160GB\n",
      "$20.00\n",
      "4\n",
      "320GB\n",
      "$40.00\n",
      "8\n",
      "640GB\n",
      "$80.00\n",
      "Pro Account\n",
      "PRO\n",
      "A monthly subscription to access powerful features.\n",
      "‚Üí\n",
      "Get Pro\n",
      "($9/month)\n",
      "ZeroGPU\n",
      ": Get 5x usage quota and highest GPU queue priority\n",
      "Spaces Hosting\n",
      ": Create ZeroGPU Spaces with A100 hardware\n",
      "Spaces Dev Mode\n",
      ": Fast iterations via SSH/VS Code for Spaces\n",
      "Inference Providers\n",
      ": Get $2 included credits across all Inference Providers\n",
      "Dataset Viewer\n",
      ": Activate it on private datasets\n",
      "Blog Articles\n",
      ": Publish articles to the Hugging Face blog\n",
      "Social Posts\n",
      ": Share short updates with the community\n",
      "Features Preview\n",
      ": Get early access to upcoming\n",
      "\t\t\t\t\t\t\t\t\t\tfeatures\n",
      "PRO\n",
      "Badge\n",
      ":\n",
      "\t\t\t\t\t\t\t\t\t\tShow your support on your profile\n",
      "System theme\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Tasks\n",
      "Inference Endpoints\n",
      "HuggingChat\n",
      "Company\n",
      "About\n",
      "Brand assets\n",
      "Terms of service\n",
      "Privacy\n",
      "Jobs\n",
      "Press\n",
      "Resources\n",
      "Learn\n",
      "Documentation\n",
      "Blog\n",
      "Forum\n",
      "Service Status\n",
      "Social\n",
      "GitHub\n",
      "Twitter\n",
      "LinkedIn\n",
      "Discord\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "404 ‚Äì Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "404\n",
      "Sorry, we can't find the page you are looking for.\n",
      "System theme\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Tasks\n",
      "Inference Endpoints\n",
      "HuggingChat\n",
      "Company\n",
      "About\n",
      "Brand assets\n",
      "Terms of service\n",
      "Privacy\n",
      "Jobs\n",
      "Press\n",
      "Resources\n",
      "Learn\n",
      "Documentation\n",
      "Blog\n",
      "Forum\n",
      "Service Status\n",
      "Social\n",
      "GitHub\n",
      "Twitter\n",
      "LinkedIn\n",
      "Discord\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "404 ‚Äì Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Posts\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "404\n",
      "Sorry, we can't find the page you are looking for.\n",
      "System theme\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Tasks\n",
      "Inference Endpoints\n",
      "HuggingChat\n",
      "Company\n",
      "About\n",
      "Brand assets\n",
      "Terms of service\n",
      "Privacy\n",
      "Jobs\n",
      "Press\n",
      "Resources\n",
      "Learn\n",
      "Documentation\n",
      "Blog\n",
      "Forum\n",
      "Service Status\n",
      "Social\n",
      "GitHub\n",
      "Twitter\n",
      "LinkedIn\n",
      "Discord\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_all_details(url):\n",
    "    result = \"Landing page:\\n\"\n",
    "    result += Website(url).get_contents()\n",
    "    \n",
    "    # Get the links\n",
    "    links = get_links(url)\n",
    "    print(\"Found links:\", links)\n",
    "    \n",
    "    base_url = \"https://huggingface.co\"  # Base URL to prepend for relative links\n",
    "    \n",
    "    # Loop through each link and retrieve its contents\n",
    "    for link in links[\"links\"]:\n",
    "        result += f\"\\n\\n{link['type']}\\n\"\n",
    "        \n",
    "        # Construct the full URL\n",
    "        full_url = urljoin(base_url, link[\"url\"])\n",
    "        \n",
    "        # Fetch the content of the full URL\n",
    "        result += Website(full_url).get_contents()\n",
    "        \n",
    "    return result\n",
    "\n",
    "# Test the function\n",
    "print(get_all_details(\"https://huggingface.co\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5099bd14-076d-4745-baf3-dac08d8e5ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_all_details(\"https://huggingface.co\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b863a55-f86c-4e3f-8a79-94e24c1a8cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the contents of several relevant pages from a company website \\\n",
    "and creates a short brochure about the company for prospective customers, investors and recruits. Respond in markdown.\\\n",
    "Include details of company culture, customers and careers/jobs if you have the information.\"\n",
    "\n",
    "# Or uncomment the lines below for a more humorous brochure - this demonstrates how easy it is to incorporate 'tone':\n",
    "\n",
    "# system_prompt = \"You are an assistant that analyzes the contents of several relevant pages from a company website \\\n",
    "# and creates a short humorous, entertaining, jokey brochure about the company for prospective customers, investors and recruits. Respond in markdown.\\\n",
    "# Include details of company culture, customers and careers/jobs if you have the information.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab83d92-d36b-4ce0-8bcc-5bb4c2f8ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brochure_user_prompt(company_name, url):\n",
    "    user_prompt = f\"You are looking at a company called: {company_name}\\n\"\n",
    "    user_prompt += f\"Here are the contents of its landing page and other relevant pages; use this information to build a short brochure of the company in markdown.\\n\"\n",
    "    user_prompt += get_all_details(url)\n",
    "    user_prompt = user_prompt[:5_000] # Truncate if more than 5,000 characters\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd909e0b-1312-4ce2-a553-821e795d7572",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_brochure_user_prompt(\"HuggingFace\", \"https://huggingface.co\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44de579-4a1a-4e6a-a510-20ea3e4b8d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_brochure(company_name, url):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_brochure_user_prompt(company_name, url)}\n",
    "          ],\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093444a-9407-42ae-924a-145730591a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_brochure(\"HuggingFace\", \"https://huggingface.co\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eaaab7-0b47-4b29-82d4-75d474ad8d18",
   "metadata": {},
   "source": [
    "## Finally - a minor improvement\n",
    "\n",
    "With a small adjustment, we can change this so that the results stream back from OpenAI,\n",
    "with the familiar typewriter animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51db0e49-f261-4137-aabe-92dd601f7725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_brochure(company_name, url):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_brochure_user_prompt(company_name, url)}\n",
    "          ],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "56bf0ae3-ee9d-4a72-9cd6-edcac67ceb6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mstream_brochure\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHuggingFace\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://huggingface.co\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mstream_brochure\u001b[39m\u001b[34m(company_name, url)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstream_brochure\u001b[39m(company_name, url):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     stream = \u001b[43mopenai\u001b[49m.chat.completions.create(\n\u001b[32m      3\u001b[39m         model=MODEL,\n\u001b[32m      4\u001b[39m         messages=[\n\u001b[32m      5\u001b[39m             {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: system_prompt},\n\u001b[32m      6\u001b[39m             {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: get_brochure_user_prompt(company_name, url)}\n\u001b[32m      7\u001b[39m           ],\n\u001b[32m      8\u001b[39m         stream=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      9\u001b[39m     )\n\u001b[32m     11\u001b[39m     response = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m     display_handle = display(Markdown(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m), display_id=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'openai' is not defined"
     ]
    }
   ],
   "source": [
    "stream_brochure(\"HuggingFace\", \"https://huggingface.co\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fdb3f8d8-a3eb-41c8-b1aa-9f60686a653b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Try changing the system prompt to the humorous version when you make the Brochure for Hugging Face:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mstream_brochure\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHuggingFace\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://huggingface.co\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mstream_brochure\u001b[39m\u001b[34m(company_name, url)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstream_brochure\u001b[39m(company_name, url):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     stream = \u001b[43mopenai\u001b[49m.chat.completions.create(\n\u001b[32m      3\u001b[39m         model=MODEL,\n\u001b[32m      4\u001b[39m         messages=[\n\u001b[32m      5\u001b[39m             {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: system_prompt},\n\u001b[32m      6\u001b[39m             {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: get_brochure_user_prompt(company_name, url)}\n\u001b[32m      7\u001b[39m           ],\n\u001b[32m      8\u001b[39m         stream=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      9\u001b[39m     )\n\u001b[32m     11\u001b[39m     response = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m     display_handle = display(Markdown(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m), display_id=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'openai' is not defined"
     ]
    }
   ],
   "source": [
    "# Try changing the system prompt to the humorous version when you make the Brochure for Hugging Face:\n",
    "\n",
    "stream_brochure(\"HuggingFace\", \"https://huggingface.co\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27bf9e0-665f-4645-b66b-9725e2a959b5",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business applications</h2>\n",
    "            <span style=\"color:#181;\">In this exercise we extended the Day 1 code to make multiple LLM calls, and generate a document.\n",
    "\n",
    "This is perhaps the first example of Agentic AI design patterns, as we combined multiple calls to LLMs. This will feature more in Week 2, and then we will return to Agentic AI in a big way in Week 8 when we build a fully autonomous Agent solution.\n",
    "\n",
    "Generating content in this way is one of the very most common Use Cases. As with summarization, this can be applied to any business vertical. Write marketing content, generate a product tutorial from a spec, create personalized email content, and so much more. Explore how you can apply content generation to your business, and try making yourself a proof-of-concept prototype. See what other students have done in the community-contributions folder -- so many valuable projects -- it's wild!</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b2454b-8ef8-4b5c-b928-053a15e0d553",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you move to Week 2 (which is tons of fun)</h2>\n",
    "            <span style=\"color:#900;\">Please see the week1 EXERCISE notebook for your challenge for the end of week 1. This will give you some essential practice working with Frontier APIs, and prepare you well for Week 2.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b64f0f-7d33-4493-985a-033d06e8db08",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">A reminder on 3 useful resources</h2>\n",
    "            <span style=\"color:#f71;\">1. The resources for the course are available <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">here.</a><br/>\n",
    "            2. I'm on LinkedIn <a href=\"https://www.linkedin.com/in/eddonner/\">here</a> and I love connecting with people taking the course!<br/>\n",
    "            3. I'm trying out X/Twitter and I'm at <a href=\"https://x.com/edwarddonner\">@edwarddonner<a> and hoping people will teach me how it's done..  \n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48e42e-fa7a-495f-a5d4-26bfc24d60b6",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../thankyou.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#090;\">Finally! I have a special request for you</h2>\n",
    "            <span style=\"color:#090;\">\n",
    "                My editor tells me that it makes a MASSIVE difference when students rate this course on Udemy - it's one of the main ways that Udemy decides whether to show it to others. If you're able to take a minute to rate this, I'd be so very grateful! And regardless - always please reach out to me at ed@edwarddonner.com if I can help at any point.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3e1a1-ba54-4907-97c5-30f89a24775b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
